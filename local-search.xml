<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Implicit Geometric Regularization</title>
    <link href="/2022/07/04/IGR/"/>
    <url>/2022/07/04/IGR/</url>
    
    <content type="html"><![CDATA[<h1 id="IGR-Implicit-Geometric-Regularization-for-Learning-Shapes"><a href="#IGR-Implicit-Geometric-Regularization-for-Learning-Shapes" class="headerlink" title="IGR: Implicit Geometric Regularization for Learning Shapes"></a><a href="https://arxiv.org/abs/2002.10099">IGR: Implicit Geometric Regularization for Learning Shapes</a></h1><p>这篇发表于ICML 2020的文章介绍了一种对隐式表面表征的约束，从效果来看也是十分惊艳的，并且文章的方法也有说服力。</p><p><img src="/./2022/07/04/IGR/2D_IGR.jpg" alt="2D IGR Result"></p><h1 id="初读和困惑"><a href="#初读和困惑" class="headerlink" title="初读和困惑"></a>初读和困惑</h1><p>输入为有向或无向点云（即是否带法向信息），训练MLP来判断Volume中每一个点的数值是多少。框架非常简洁明了，关键在于这个损失函数的构造。</p><p>$$<br>l(\theta)&#x3D;l_{\chi}(\theta) + \lambda E_x(||\nabla_xf(x;\theta)||-1)^2<br>$$</p><p>上面的式子为文章提出的损失函数，其中$\lambda &gt; 0$，是一个人为设置的超参数，$||\cdot||&#x3D;||\cdot||_2$是欧几里德距离，第一项$l_\chi(\theta)$为</p><p>$$<br>l_\chi(\theta)&#x3D;\frac{1}{|I|}\Sigma){i\in I}(f(x_i;\theta)+\tau||\nabla_xf(x_i;\theta)-n_i||)<br>$$</p><p>这一项的目的是为了当输入是有向点云即点云包含法向信息时，增加一个点$x$的发现约束。</p><p>而$\nabla$算子，文中表示它代表着程函方程，即</p><p>$$<br>||\nabla_xf(x)||&#x3D;1<br>$$</p><p>一开始看到这就很懵了，程函方程不是电磁学的内容吗？咋用在这个损失函数里了？它代表啥意思？</p><h1 id="再读与理解"><a href="#再读与理解" class="headerlink" title="再读与理解"></a>再读与理解</h1><p>我们抛开文中定义的程函方程，单看$\nabla\cdot$算子，他其实代表的也是梯度的计算，比如在Poisson重建中用到的解Poison方程也是用的$\nabla$算子。</p><p>那我们从梯度的角度来分析，程函方程这一个loss约束项本质上就是让表面附近的梯度为常数，即尽可能的让表面变化保持一致。</p><p><img src="/./2022/07/04/IGR/3D_IGR.png" alt="3D IGR Result"></p><h1 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h1><p>之后文章证明了这个梯度方程也可以直接放在PyTorch中进行对应的计算和回传，因此可以很方便的将它嵌入到相应的任务中。</p><p>总体而言，IGR和Possion表面重建有异曲同工之处，利用曲面平滑约束来完成点云的曲面重建，并且IGR将其带入到了神经网络中来完成这项任务。</p><p><a href="https://arxiv.org/abs/2106.03452">Shape As Points</a>这篇文章则是用可微分的Poisson求解器来求解Poisson表面重建问题，后续有时间再详细介绍一下这篇工作。</p><p>IGR是一项优秀的工作，它易于嵌入并且在表征上有着合理的解释，在文章中也显示有很好的重建结果，是非常solid的工作。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://arxiv.org/abs/2002.10099">IGR Paper</a></p><p><a href="https://github.com/amosgropp/IGR">IGR Codebase</a></p>]]></content>
    
    
    <categories>
      
      <category>paper reading</category>
      
    </categories>
    
    
    <tags>
      
      <tag>volume reconstruction</tag>
      
      <tag>implicit function</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2D图像的傅里叶变换</title>
    <link href="/2022/06/30/2DFourierTransform/"/>
    <url>/2022/06/30/2DFourierTransform/</url>
    
    <content type="html"><![CDATA[<h1 id="图像的频域信息"><a href="#图像的频域信息" class="headerlink" title="图像的频域信息"></a>图像的频域信息</h1><p>最近在看一些文章的时候，采用了谱方法来解Poisson Equation，大部分采用的谱方法是用傅里叶变换来完成。同样在一些文章中将图片转换为频域信息进行分析，也是使用了傅里叶变换将空域图像转化为频域。因此稍微深入了解了一下二维图像的傅里叶变换。</p><p>在进入2D傅里叶变换前，我们先要回顾一下一维的傅里叶变换。</p><h1 id="傅里叶变换"><a href="#傅里叶变换" class="headerlink" title="傅里叶变换"></a>傅里叶变换</h1><p>傅里叶变换，Fourier Transformation，是一种线性积分变换，常用于信号处理。它指的是一个一维的信号<br>可以被分解成若干个复指数波$e^{i\theta}$，而有欧拉公式$e^{i\theta}&#x3D;\cos \theta + i\cdot \sin \theta$，因此每一个复指数波都是实部余弦波和虚部正弦波组成。</p><p>对于每一个正弦波（余弦波），都有频率、相位和振幅来决定，因此在频域中一维代表频率，每一个坐标对应的函数值是一个复数，复数的幅度代表振幅，相位角代表相位。</p><p>实际上一维傅里叶变换就是把一个复杂的波函数转化为一系列正弦余弦正交基的过程，如下图所示。</p><p><img src="/./2022/06/30/2DFourierTransform/1DFourier.jpg" alt="1D Fourier Transform"></p><h1 id="图像的傅里叶变换"><a href="#图像的傅里叶变换" class="headerlink" title="图像的傅里叶变换"></a>图像的傅里叶变换</h1><p>那么在二维图片中，傅里叶变换实际上就是将一个图像分解成若干个复平面波$e^{j2\pi(ux+vy)}$之和，每一个复平面波就是二维变换的一个基函数。</p><p>二维傅里叶变换的公式如下：</p><p>$$<br>F(u,v) &#x3D; \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}f(x,y)e^{-j2\pi(ux+vy)}dxdy<br>$$</p><p>其中$F(u,v)$为变换后的值，$f(x,y)$为图像像素值，$u,v$为变换后图像的坐标，$x,y$为原图像的坐标，$j$表示复数。</p><p>但上述式子是在连续的空间中做的处理，我们在处理图像时实际上是处理的光栅化后的图片（即以像素为单位），因此我们真正使用的是二维离散傅里叶变换（2D Discrete Fourier Transform）。</p><p>我们以像素为单位进行采样，则有傅里叶变换<br>$$<br>F(u,v) &#x3D; \Sigma_{x&#x3D;0}^{M-1}\Sigma_{y&#x3D;0}^{N-1}f(x,y)e^{-j2\pi(\frac{ux}{M}+\frac{vy}{N})}<br>$$</p><p>逆傅里叶变换：</p><p>$$<br>f(x,y) &#x3D; \Sigma_{u&#x3D;0}^{M-1}\Sigma_{v&#x3D;0}^{N-1}F(u,v)e^{j2\pi(\frac{ux}{M}+\frac{vy}{N})}<br>$$</p><p>至此，我们可以用上述公式对图片进行傅里叶变换和逆傅里叶变换。另外需要注意的是，在这里我们输入的都是灰度图片。</p><h1 id="二维DFT的python实现"><a href="#二维DFT的python实现" class="headerlink" title="二维DFT的python实现"></a>二维DFT的python实现</h1><p>DFT的实现如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@jit</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">naive_DFT</span>(<span class="hljs-params">image</span>):<br>    <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(image.shape) == <span class="hljs-number">2</span><br>    H, W = image.shape<br>    <span class="hljs-comment"># Prepare DFT coefficient</span><br>    G = np.zeros_like(image, dtype=np.complex128)<br>    <br>    <span class="hljs-comment"># prepare processed index corresponding to original image positions</span><br>    x = np.tile(np.arange(W), (H, <span class="hljs-number">1</span>))<br>    y = np.arange(H).repeat(W).reshape(H, -<span class="hljs-number">1</span>)<br>    <br><br>    <span class="hljs-comment"># DFT</span><br>    <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(H):<br>        <span class="hljs-keyword">for</span> u <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(W):<br>            G[v, u] = np.<span class="hljs-built_in">sum</span>(image * np.exp(-<span class="hljs-number">2j</span> * np.pi * (x * u / W + y * v / H))) / np.sqrt(H * W)<br><br>    <span class="hljs-comment"># shift to recenter image</span><br>    shift_G = np.concatenate([G[H//<span class="hljs-number">2</span>:,:], G[:H//<span class="hljs-number">2</span>, :]], axis=<span class="hljs-number">0</span>)<br>    shift_G = np.concatenate([shift_G[:,W//<span class="hljs-number">2</span>:], shift_G[:, :W//<span class="hljs-number">2</span>]], axis=<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> shift_G <span class="hljs-comment"># show img using np.log(np.abs(shift_G))</span><br></code></pre></td></tr></table></figure><p>iDFT的实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@jit</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">naive_iDFT</span>(<span class="hljs-params">fshift</span>):<br>    <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(fshift.shape) == <span class="hljs-number">2</span><br>    H, W = fshift.shape<br><br>    <span class="hljs-comment"># shift for inverse DFT caculation</span><br>    fshift = np.concatenate([fshift[(H+<span class="hljs-number">1</span>)//<span class="hljs-number">2</span>:,:], fshift[:(H+<span class="hljs-number">1</span>)//<span class="hljs-number">2</span>, :]], axis=<span class="hljs-number">0</span>)<br>    fshift = np.concatenate([fshift[:,(W+<span class="hljs-number">1</span>)//<span class="hljs-number">2</span>:], fshift[:, :(W+<span class="hljs-number">1</span>)//<span class="hljs-number">2</span>]], axis=<span class="hljs-number">1</span>)<br>    <br>    <span class="hljs-comment"># Prepare iDFT coefficient</span><br>    G = np.zeros_like(fshift, dtype=np.float32)<br>    <br>    <span class="hljs-comment"># prepare processed index corresponding to original image positions</span><br>    x = np.tile(np.arange(W), (H, <span class="hljs-number">1</span>))<br>    y = np.arange(H).repeat(W).reshape(H, -<span class="hljs-number">1</span>)<br>    <br><br>    <span class="hljs-comment"># iDFT</span><br>    <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(H):<br>        <span class="hljs-keyword">for</span> u <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(W):<br>            G[v, u] = np.<span class="hljs-built_in">sum</span>(fshift * np.exp(<span class="hljs-number">2j</span> * np.pi * (x * u / W + y * v / H))) / np.sqrt(H * W)<br><br>    <span class="hljs-keyword">return</span> np.<span class="hljs-built_in">abs</span>(G)<br></code></pre></td></tr></table></figure><p>同样，我们可以使用numpy中实现的FFT来完成相应的操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">numpy_FFT2</span>(<span class="hljs-params">image</span>):<br>    <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(image.shape) == <span class="hljs-number">2</span><br>    ft = np.fft.fft2(image)<br>    fshift = np.fft.fftshift(ft)<br>    <span class="hljs-keyword">return</span> fshift <span class="hljs-comment"># show img using np.log(np.abs(fshift))</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">numpy_iFFT2</span>(<span class="hljs-params">fshift</span>):<br>    <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(fshift.shape) == <span class="hljs-number">2</span><br>    iffshift = np.fft.ifftshift(fshift)<br>    image = np.fft.ifft2(iffshift)<br>    <span class="hljs-keyword">return</span> np.<span class="hljs-built_in">abs</span>(image)<br></code></pre></td></tr></table></figure><p>注意到这里面还有一个shift的操作，是因为我们希望得到中心为原点的频域图，而计算的时候并不是以图片中心为原点开始计算的，因此需要加上一个相位偏移。如果没有shift的操作得到的是如下的结果：<br><img src="/./2022/06/30/2DFourierTransform/Figure_2.png" alt="2D Fourier Transform without Shift"></p><p>其中第一行为numpy的FFT得到的结果，第二行为我们实现的二维离散傅里叶变换。第一列为原图，第二列为频谱图，第三列为用频谱图经过逆傅里叶变换恢复的图像。可以看出实现与numpy的结果没有差别。</p><p>加上了相位偏移，将原点移动至中心后，结果如下：<br><img src="/./2022/06/30/2DFourierTransform/Figure_1.png" alt="2D Fourier Transform with Shift"></p><p>这时的图像一般就是我们需要去分析或使用的图像。</p><p>到现在，我们完成了二维图片的傅里叶变换（2D DFT），研究人员们为了加速计算，利用一些数学方法加速后提出快速傅里叶变换即FFT，极大的加速了计算时间，本文探讨傅里叶变换的原理和过程，在此不对FFT进一步说明。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>Math is beautiful. 傅里叶变换能够应用到很多的地方，一些数学方法还是需要我们仔细去学习推敲一下。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://baike.baidu.com/item/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2/7119029?fr=aladdin">傅里叶变换 - 百度百科</a></p><p><a href="https://www.zhihu.com/question/22611929">二维傅里叶变换是怎么进行的？- 阿姆斯特朗的回答</a></p>]]></content>
    
    
    <categories>
      
      <category>math</category>
      
    </categories>
    
    
    <tags>
      
      <tag>math</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SGM</title>
    <link href="/2022/06/28/SGM/"/>
    <url>/2022/06/28/SGM/</url>
    
    <content type="html"><![CDATA[<h1 id="Semi-Global-Matching"><a href="#Semi-Global-Matching" class="headerlink" title="Semi-Global Matching"></a>Semi-Global Matching</h1><p>Semi-Global Matching，即半全局匹配，是双目立体匹配中一项非常出色的工作。SGM由Heiko Hirschmuller于2007年在T-PAMI上发表的文章<a href="https://elib.dlr.de/55367/1/Stereo_Processing-Hirschm%c3%bcller.pdf">Stereo Processing by Semi-Global Matching and Mutual Information</a>提出。Semi-Global顾名思义，是介于全局和局部之间的一种算法，既没有考虑整张图像所有的像素点，也没有只考虑像素的局部区域。</p><h1 id="熵与互信息"><a href="#熵与互信息" class="headerlink" title="熵与互信息"></a>熵与互信息</h1><p>SGM文章中采用的cost计算是基于熵和互信息来完成的。熵$H_I$、联合熵$H_{I_1,I_2}$和互信息$MI_{I_1,I_2}$如下公式定义。</p><p>$$<br>H_I &#x3D; - \int_0^1P_I(i)\log P_I(i)di<br>$$</p><p>$$<br>H_{I_1,I_2} &#x3D; - \int_0^1\int_0^1P_{I_1,I_2}(i_1,i_2) \log P_{I_1,I_2}(i_1,i_2)di_1di_2<br>$$</p><p>$$<br>MI_{I_1,I_2}&#x3D;H_{I_1} + H_{I_2} - H_{I_1,I_2}<br>$$</p><p>其中$P$为值出现的概率。文章指出利用互信息的方式来表示pixel对应的cost值，对于光强等有良好的鲁棒性。</p><p>那么在图片中如何计算呢？SGM的输入是左右两张<strong>极线矫正后</strong>的<strong>灰度图</strong>，因此首先计算每一张图片的灰度直方图，以此来表示每一个灰度值出现的概率，所以单张图片中的概率函数自变量的取值是0<del>255。而图片的联合概率$P_{I_1,I_2}(i,k)$则定义域(0,0)</del>(255,255)，定义为：</p><p>$$<br>P_{I_1,I_2}(i,k) &#x3D; \frac{1}{n} \Sigma_pT[(i,k)&#x3D;&#x3D;(I_{1p},I_{2p})]<br>$$</p><p>其中$T$为指示函数，条件为真值为1否则为0。那么实际上在预处理的时候我们只需要将256*256的联合概率图计算好就行。另外还对这个概率图做了Gaussian平滑处理，目的是为了减少噪声对结果的影响。</p><h1 id="代价聚合"><a href="#代价聚合" class="headerlink" title="代价聚合"></a>代价聚合</h1><p>我们已经可以算出基于MI的匹配代价，但是这只是per-pixel level的处理，很容易受到误差和噪声的影响。因此SGM还需要考虑使用邻域信息来构造loss function对其进行约束。文章使用能量公式$E(D)$来表示。</p><p>$$<br>E(D)&#x3D;\Sigma_p(C(p,D_p)+\Sigma_{q\in N_p}P_1T[|D_p-D_q|&#x3D;1]+\Sigma_{q\in N_p}P_2T[|D_p-D_q|&gt;1])<br>$$</p><p>其中$P_1,P_2$都是人为设置的惩罚项常数，后两项是当前像素$p$和其邻域$N_p$内所有像素$q$之间的约束。现在问题就转化为找出一个最终的Disparity Map使得这个全局能量函数最小。</p><p>文章提出每一个点都有周围的N个（8个、16个等）方向，我们只需要将没一个方向上进行传播计算就能够让对点$p$不可导的$E(D)$利用这种方式来进行优化。</p><p>$$<br>L_r(p,d)&#x3D;C(p,d) + \min(L_r(p-r,d), L_r(p-r,d-1)+P_1, L_r(p-r,d+1)+P_1, \min_iL_r(p-r,i)+P_2) - \min_kL_r(p-r,k)<br>$$</p><p>$p$点上某个邻域方向的代价聚合包含：当前点的匹配代价；min（当前邻域方向上$p-r$这个像素点的当前视差代价聚合，$p-r$点的视差差值为1的代价聚合$+P_1$，$p-r$点的视差值大于1的最小代价聚合$+P_2$）；$p-r$点的视差插值大于1的最小代价聚合，last term是防止结果过大做的处理。</p><p>而$p-r$实际上就是沿着某一个方向从前往后计算直到结束。</p><h1 id="关于部分实现"><a href="#关于部分实现" class="headerlink" title="关于部分实现"></a>关于部分实现</h1><p>在实现SGM的时候，文章中说是随机初始化几次来做初值，但是一般的做法是在Disp范围内都去计算互信息得到最初的Cost Volume，然后在对不同方向上做代价聚合。</p><h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>SGM是一个很强大的方法，在Deep Learning没开战以前榜单前列都是基于SGM的改进，OpenCV也实现了SGBM的双目立体匹配算法，同时也有很多基于CUDA的并行实现，让SGM算法有了更快的运行时间，是双目立体匹配中经典的算法。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://elib.dlr.de/55367/1/Stereo_Processing-Hirschm%c3%bcller.pdf">Stereo Processing by Semi-Global Matching and Mutual Information</a></p><p><a href="https://blog.csdn.net/wsj998689aa/article/details/49464017">CSDN-王嗣钧 blog</a></p><p><a href="https://blog.csdn.net/zilanpotou182/article/details/73382412">CSDN-Witnesses blog</a></p>]]></content>
    
    
    <categories>
      
      <category>3D Algorithm</category>
      
    </categories>
    
    
    <tags>
      
      <tag>stereo matching</tag>
      
      <tag>traditional</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Diving into NeRF-PyTorch</title>
    <link href="/2022/06/27/nerf-pytorch/"/>
    <url>/2022/06/27/nerf-pytorch/</url>
    
    <content type="html"><![CDATA[<h1 id="Diving-into-NeRF-PyTorch"><a href="#Diving-into-NeRF-PyTorch" class="headerlink" title="Diving into NeRF-PyTorch"></a>Diving into NeRF-PyTorch</h1><p>NeRF，即Neural Radiance Fields，由Ben Mildenhall等人于ECCV2020在文章<a href="https://arxiv.org/pdf/2003.08934.pdf">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</a>中提出。</p><p>本质上，NeRF和DeepSDF类似，都是将信息encode到神经网络中，而NeRF利用Differentiable Volume Rendering和positional encoding将整个过程变得可微，然后可以利用神经网络来进行bp。</p><h1 id="A-Brief-Review-of-NeRF"><a href="#A-Brief-Review-of-NeRF" class="headerlink" title="A Brief Review of NeRF"></a>A Brief Review of NeRF</h1><ul><li>利用已知相机参数的图片组进行训练MLP</li><li>MLP输入为3D点$&lt;x,y,z&gt;$和方向$&lt;\theta, \phi&gt;$，输出为该RGB和density</li><li>加入了3D点的positional encoding</li><li>利用可微的volume rendering在图片视角下进行渲染并利用输入的图片组进行约束学习</li></ul><h1 id="Implementation-Keypoints"><a href="#Implementation-Keypoints" class="headerlink" title="Implementation Keypoints"></a>Implementation Keypoints</h1><p>本文代码阅读基于<a href="https://github.com/yenchenlin/nerf-pytorch">NeRF-PyTorch Codebase</a>。</p><h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p>位置编码如同paper中介绍的一样，利用$sin$和$cos$对位置信息进行编码，利用不同频率的算子对其进行解耦。另在NeRF中，$&lt;x,y,z&gt;$三个方向是分别进行位置编码的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Positional encoding (section 5.1)</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Embedder</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, **kwargs</span>):<br>        self.kwargs = kwargs<br>        self.create_embedding_fn()<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">create_embedding_fn</span>(<span class="hljs-params">self</span>):<br>        embed_fns = []<br>        d = self.kwargs[<span class="hljs-string">&#x27;input_dims&#x27;</span>]<br>        out_dim = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">if</span> self.kwargs[<span class="hljs-string">&#x27;include_input&#x27;</span>]:<br>            embed_fns.append(<span class="hljs-keyword">lambda</span> x : x)<br>            out_dim += d<br>            <br>        max_freq = self.kwargs[<span class="hljs-string">&#x27;max_freq_log2&#x27;</span>]<br>        N_freqs = self.kwargs[<span class="hljs-string">&#x27;num_freqs&#x27;</span>]<br>        <br>        <span class="hljs-keyword">if</span> self.kwargs[<span class="hljs-string">&#x27;log_sampling&#x27;</span>]:<br>            freq_bands = <span class="hljs-number">2.</span>**torch.linspace(<span class="hljs-number">0.</span>, max_freq, steps=N_freqs)<br>        <span class="hljs-keyword">else</span>:<br>            freq_bands = torch.linspace(<span class="hljs-number">2.</span>**<span class="hljs-number">0.</span>, <span class="hljs-number">2.</span>**max_freq, steps=N_freqs)<br>            <br>        <span class="hljs-keyword">for</span> freq <span class="hljs-keyword">in</span> freq_bands:<br>            <span class="hljs-keyword">for</span> p_fn <span class="hljs-keyword">in</span> self.kwargs[<span class="hljs-string">&#x27;periodic_fns&#x27;</span>]:<br>                embed_fns.append(<span class="hljs-keyword">lambda</span> x, p_fn=p_fn, freq=freq : p_fn(x * freq))<br>                out_dim += d<br>                    <br>        self.embed_fns = embed_fns<br>        self.out_dim = out_dim<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">embed</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-keyword">return</span> torch.cat([fn(inputs) <span class="hljs-keyword">for</span> fn <span class="hljs-keyword">in</span> self.embed_fns], -<span class="hljs-number">1</span>)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_embedder</span>(<span class="hljs-params">multires, i=<span class="hljs-number">0</span></span>):<br>    <span class="hljs-keyword">if</span> i == -<span class="hljs-number">1</span>:<br>        <span class="hljs-keyword">return</span> nn.Identity(), <span class="hljs-number">3</span><br>    <br>    embed_kwargs = &#123;<br>                <span class="hljs-string">&#x27;include_input&#x27;</span> : <span class="hljs-literal">True</span>,<br>                <span class="hljs-string">&#x27;input_dims&#x27;</span> : <span class="hljs-number">3</span>,<br>                <span class="hljs-string">&#x27;max_freq_log2&#x27;</span> : multires-<span class="hljs-number">1</span>,<br>                <span class="hljs-string">&#x27;num_freqs&#x27;</span> : multires,<br>                <span class="hljs-string">&#x27;log_sampling&#x27;</span> : <span class="hljs-literal">True</span>,<br>                <span class="hljs-string">&#x27;periodic_fns&#x27;</span> : [torch.sin, torch.cos],<br>    &#125;<br>    <br>    embedder_obj = Embedder(**embed_kwargs)<br>    embed = <span class="hljs-keyword">lambda</span> x, eo=embedder_obj : eo.embed(x)<br>    <span class="hljs-keyword">return</span> embed, embedder_obj.<br></code></pre></td></tr></table></figure><h2 id="Get-Rays"><a href="#Get-Rays" class="headerlink" title="Get Rays"></a>Get Rays</h2><p>一开始对这个direction的获取没有理解，后面在issue中看到这篇文章：<a href="https://www.scratchapixel.com/lessons/3d-basic-rendering/ray-tracing-generating-camera-rays/generating-camera-rays">Ray-Tracing: Generating Camera Rays</a>后有了进一步的认识。实际还是3D Basic没有了解到位，概括一下实际上这个direction就是在ndc坐标系下，相机在原点而ndc平面在1个单位距离处对每一个pixel计算方向得到。然后将direction都和外参矩阵相乘就能够得到世界坐标系下每一个pixel对应的ray的direction，也能够知道相机原点的position。之后只需要按照步长向前pass来做volume render就可以了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Ray helpers</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_rays</span>(<span class="hljs-params">H, W, K, c2w</span>):<br>    i, j = torch.meshgrid(torch.linspace(<span class="hljs-number">0</span>, W-<span class="hljs-number">1</span>, W), torch.linspace(<span class="hljs-number">0</span>, H-<span class="hljs-number">1</span>, H))  <span class="hljs-comment"># pytorch&#x27;s meshgrid has indexing=&#x27;ij&#x27;</span><br>    i = i.t()<br>    j = j.t()<br>    dirs = torch.stack([(i-K[<span class="hljs-number">0</span>][<span class="hljs-number">2</span>])/K[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>], -(j-K[<span class="hljs-number">1</span>][<span class="hljs-number">2</span>])/K[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>], -torch.ones_like(i)], -<span class="hljs-number">1</span>)<br>    <span class="hljs-comment"># Rotate ray directions from camera frame to the world frame</span><br>    rays_d = torch.<span class="hljs-built_in">sum</span>(dirs[..., np.newaxis, :] * c2w[:<span class="hljs-number">3</span>,:<span class="hljs-number">3</span>], -<span class="hljs-number">1</span>)  <span class="hljs-comment"># dot product, equals to: [c2w.dot(dir) for dir in dirs]</span><br>    <span class="hljs-comment"># Translate camera frame&#x27;s origin to the world frame. It is the origin of all rays.</span><br>    rays_o = c2w[:<span class="hljs-number">3</span>,-<span class="hljs-number">1</span>].expand(rays_d.shape)<br>    <span class="hljs-keyword">return</span> rays_o, rays_d<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_rays_np</span>(<span class="hljs-params">H, W, K, c2w</span>):<br>    i, j = np.meshgrid(np.arange(W, dtype=np.float32), np.arange(H, dtype=np.float32), indexing=<span class="hljs-string">&#x27;xy&#x27;</span>)<br>    dirs = np.stack([(i-K[<span class="hljs-number">0</span>][<span class="hljs-number">2</span>])/K[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>], -(j-K[<span class="hljs-number">1</span>][<span class="hljs-number">2</span>])/K[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>], -np.ones_like(i)], -<span class="hljs-number">1</span>)<br>    <span class="hljs-comment"># Rotate ray directions from camera frame to the world frame</span><br>    rays_d = np.<span class="hljs-built_in">sum</span>(dirs[..., np.newaxis, :] * c2w[:<span class="hljs-number">3</span>,:<span class="hljs-number">3</span>], -<span class="hljs-number">1</span>)  <span class="hljs-comment"># dot product, equals to: [c2w.dot(dir) for dir in dirs]</span><br>    <span class="hljs-comment"># Translate camera frame&#x27;s origin to the world frame. It is the origin of all rays.</span><br>    rays_o = np.broadcast_to(c2w[:<span class="hljs-number">3</span>,-<span class="hljs-number">1</span>], np.shape(rays_d))<br>    <span class="hljs-keyword">return</span> rays_o, rays_d<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ndc_rays</span>(<span class="hljs-params">H, W, focal, near, rays_o, rays_d</span>):<br>    <span class="hljs-comment"># Shift ray origins to near plane</span><br>    t = -(near + rays_o[...,<span class="hljs-number">2</span>]) / rays_d[...,<span class="hljs-number">2</span>]<br>    rays_o = rays_o + t[...,<span class="hljs-literal">None</span>] * rays_d<br>    <br>    <span class="hljs-comment"># Projection</span><br>    o0 = -<span class="hljs-number">1.</span>/(W/(<span class="hljs-number">2.</span>*focal)) * rays_o[...,<span class="hljs-number">0</span>] / rays_o[...,<span class="hljs-number">2</span>]<br>    o1 = -<span class="hljs-number">1.</span>/(H/(<span class="hljs-number">2.</span>*focal)) * rays_o[...,<span class="hljs-number">1</span>] / rays_o[...,<span class="hljs-number">2</span>]<br>    o2 = <span class="hljs-number">1.</span> + <span class="hljs-number">2.</span> * near / rays_o[...,<span class="hljs-number">2</span>]<br><br>    d0 = -<span class="hljs-number">1.</span>/(W/(<span class="hljs-number">2.</span>*focal)) * (rays_d[...,<span class="hljs-number">0</span>]/rays_d[...,<span class="hljs-number">2</span>] - rays_o[...,<span class="hljs-number">0</span>]/rays_o[...,<span class="hljs-number">2</span>])<br>    d1 = -<span class="hljs-number">1.</span>/(H/(<span class="hljs-number">2.</span>*focal)) * (rays_d[...,<span class="hljs-number">1</span>]/rays_d[...,<span class="hljs-number">2</span>] - rays_o[...,<span class="hljs-number">1</span>]/rays_o[...,<span class="hljs-number">2</span>])<br>    d2 = -<span class="hljs-number">2.</span> * near / rays_o[...,<span class="hljs-number">2</span>]<br>    <br>    rays_o = torch.stack([o0,o1,o2], -<span class="hljs-number">1</span>)<br>    rays_d = torch.stack([d0,d1,d2], -<span class="hljs-number">1</span>)<br>    <br>    <span class="hljs-keyword">return</span> rays_o, rays_d<br></code></pre></td></tr></table></figure><h2 id="Render-Rays"><a href="#Render-Rays" class="headerlink" title="Render Rays"></a>Render Rays</h2><p>NeRF中还有一个关键就是对每个pixel产生的光线渲染出最后的RGB和density。在<code>render_ray</code>函数中，从发射原点出发，对于给定的方向进行采样，这样就能够对每一个pixel对应的有限长光线(对于这个volume来说有最近距离和最远距离)进行采样。采样到的点经过NeRF定义的MLP就能够获得对应的RGB和density，对所有采样点的值进行累加即为在当前位姿下，这个pixel对应光线利用NeRF的MLP渲染出来的结果。这个过程每个点仍然是可微的，因此整个网络可以用输入的图片对应的RGB来进行约束和bp回传更新。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">render_rays</span>(<span class="hljs-params">ray_batch,</span><br><span class="hljs-params">                network_fn,</span><br><span class="hljs-params">                network_query_fn,</span><br><span class="hljs-params">                N_samples,</span><br><span class="hljs-params">                retraw=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">                lindisp=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">                perturb=<span class="hljs-number">0.</span>,</span><br><span class="hljs-params">                N_importance=<span class="hljs-number">0</span>,</span><br><span class="hljs-params">                network_fine=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">                white_bkgd=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">                raw_noise_std=<span class="hljs-number">0.</span>,</span><br><span class="hljs-params">                verbose=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">                pytest=<span class="hljs-literal">False</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Volumetric rendering.</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">      ray_batch: array of shape [batch_size, ...]. All information necessary</span><br><span class="hljs-string">        for sampling along a ray, including: ray origin, ray direction, min</span><br><span class="hljs-string">        dist, max dist, and unit-magnitude viewing direction.</span><br><span class="hljs-string">      network_fn: function. Model for predicting RGB and density at each point</span><br><span class="hljs-string">        in space.</span><br><span class="hljs-string">      network_query_fn: function used for passing queries to network_fn.</span><br><span class="hljs-string">      N_samples: int. Number of different times to sample along each ray.</span><br><span class="hljs-string">      retraw: bool. If True, include model&#x27;s raw, unprocessed predictions.</span><br><span class="hljs-string">      lindisp: bool. If True, sample linearly in inverse depth rather than in depth.</span><br><span class="hljs-string">      perturb: float, 0 or 1. If non-zero, each ray is sampled at stratified</span><br><span class="hljs-string">        random points in time.</span><br><span class="hljs-string">      N_importance: int. Number of additional times to sample along each ray.</span><br><span class="hljs-string">        These samples are only passed to network_fine.</span><br><span class="hljs-string">      network_fine: &quot;fine&quot; network with same spec as network_fn.</span><br><span class="hljs-string">      white_bkgd: bool. If True, assume a white background.</span><br><span class="hljs-string">      raw_noise_std: ...</span><br><span class="hljs-string">      verbose: bool. If True, print more debugging info.</span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">      rgb_map: [num_rays, 3]. Estimated RGB color of a ray. Comes from fine model.</span><br><span class="hljs-string">      disp_map: [num_rays]. Disparity map. 1 / depth.</span><br><span class="hljs-string">      acc_map: [num_rays]. Accumulated opacity along each ray. Comes from fine model.</span><br><span class="hljs-string">      raw: [num_rays, num_samples, 4]. Raw predictions from model.</span><br><span class="hljs-string">      rgb0: See rgb_map. Output for coarse model.</span><br><span class="hljs-string">      disp0: See disp_map. Output for coarse model.</span><br><span class="hljs-string">      acc0: See acc_map. Output for coarse model.</span><br><span class="hljs-string">      z_std: [num_rays]. Standard deviation of distances along ray for each</span><br><span class="hljs-string">        sample.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    N_rays = ray_batch.shape[<span class="hljs-number">0</span>]<br>    rays_o, rays_d = ray_batch[:,<span class="hljs-number">0</span>:<span class="hljs-number">3</span>], ray_batch[:,<span class="hljs-number">3</span>:<span class="hljs-number">6</span>] <span class="hljs-comment"># [N_rays, 3] each</span><br>    viewdirs = ray_batch[:,-<span class="hljs-number">3</span>:] <span class="hljs-keyword">if</span> ray_batch.shape[-<span class="hljs-number">1</span>] &gt; <span class="hljs-number">8</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>    bounds = torch.reshape(ray_batch[...,<span class="hljs-number">6</span>:<span class="hljs-number">8</span>], [-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>])<br>    near, far = bounds[...,<span class="hljs-number">0</span>], bounds[...,<span class="hljs-number">1</span>] <span class="hljs-comment"># [-1,1]</span><br><br>    t_vals = torch.linspace(<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, steps=N_samples)<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> lindisp:<br>        z_vals = near * (<span class="hljs-number">1.</span>-t_vals) + far * (t_vals)<br>    <span class="hljs-keyword">else</span>:<br>        z_vals = <span class="hljs-number">1.</span>/(<span class="hljs-number">1.</span>/near * (<span class="hljs-number">1.</span>-t_vals) + <span class="hljs-number">1.</span>/far * (t_vals))<br><br>    z_vals = z_vals.expand([N_rays, N_samples])<br><br>    <span class="hljs-keyword">if</span> perturb &gt; <span class="hljs-number">0.</span>:<br>        <span class="hljs-comment"># get intervals between samples</span><br>        mids = <span class="hljs-number">.5</span> * (z_vals[...,<span class="hljs-number">1</span>:] + z_vals[...,:-<span class="hljs-number">1</span>])<br>        upper = torch.cat([mids, z_vals[...,-<span class="hljs-number">1</span>:]], -<span class="hljs-number">1</span>)<br>        lower = torch.cat([z_vals[...,:<span class="hljs-number">1</span>], mids], -<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># stratified samples in those intervals</span><br>        t_rand = torch.rand(z_vals.shape)<br><br>        <span class="hljs-comment"># Pytest, overwrite u with numpy&#x27;s fixed random numbers</span><br>        <span class="hljs-keyword">if</span> pytest:<br>            np.random.seed(<span class="hljs-number">0</span>)<br>            t_rand = np.random.rand(*<span class="hljs-built_in">list</span>(z_vals.shape))<br>            t_rand = torch.Tensor(t_rand)<br><br>        z_vals = lower + (upper - lower) * t_rand<br><br>    pts = rays_o[...,<span class="hljs-literal">None</span>,:] + rays_d[...,<span class="hljs-literal">None</span>,:] * z_vals[...,:,<span class="hljs-literal">None</span>] <span class="hljs-comment"># [N_rays, N_samples, 3]</span><br><br><br><span class="hljs-comment">#     raw = run_network(pts)</span><br>    raw = network_query_fn(pts, viewdirs, network_fn)<br>    rgb_map, disp_map, acc_map, weights, depth_map = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd, pytest=pytest)<br><br>    <span class="hljs-keyword">if</span> N_importance &gt; <span class="hljs-number">0</span>:<br><br>        rgb_map_0, disp_map_0, acc_map_0 = rgb_map, disp_map, acc_map<br><br>        z_vals_mid = <span class="hljs-number">.5</span> * (z_vals[...,<span class="hljs-number">1</span>:] + z_vals[...,:-<span class="hljs-number">1</span>])<br>        z_samples = sample_pdf(z_vals_mid, weights[...,<span class="hljs-number">1</span>:-<span class="hljs-number">1</span>], N_importance, det=(perturb==<span class="hljs-number">0.</span>), pytest=pytest)<br>        z_samples = z_samples.detach()<br><br>        z_vals, _ = torch.sort(torch.cat([z_vals, z_samples], -<span class="hljs-number">1</span>), -<span class="hljs-number">1</span>)<br>        pts = rays_o[...,<span class="hljs-literal">None</span>,:] + rays_d[...,<span class="hljs-literal">None</span>,:] * z_vals[...,:,<span class="hljs-literal">None</span>] <span class="hljs-comment"># [N_rays, N_samples + N_importance, 3]</span><br><br>        run_fn = network_fn <span class="hljs-keyword">if</span> network_fine <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> network_fine<br><span class="hljs-comment">#         raw = run_network(pts, fn=run_fn)</span><br>        raw = network_query_fn(pts, viewdirs, run_fn)<br><br>        rgb_map, disp_map, acc_map, weights, depth_map = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd, pytest=pytest)<br><br>    ret = &#123;<span class="hljs-string">&#x27;rgb_map&#x27;</span> : rgb_map, <span class="hljs-string">&#x27;disp_map&#x27;</span> : disp_map, <span class="hljs-string">&#x27;acc_map&#x27;</span> : acc_map&#125;<br>    <span class="hljs-keyword">if</span> retraw:<br>        ret[<span class="hljs-string">&#x27;raw&#x27;</span>] = raw<br>    <span class="hljs-keyword">if</span> N_importance &gt; <span class="hljs-number">0</span>:<br>        ret[<span class="hljs-string">&#x27;rgb0&#x27;</span>] = rgb_map_0<br>        ret[<span class="hljs-string">&#x27;disp0&#x27;</span>] = disp_map_0<br>        ret[<span class="hljs-string">&#x27;acc0&#x27;</span>] = acc_map_0<br>        ret[<span class="hljs-string">&#x27;z_std&#x27;</span>] = torch.std(z_samples, dim=-<span class="hljs-number">1</span>, unbiased=<span class="hljs-literal">False</span>)  <span class="hljs-comment"># [N_rays]</span><br><br>    <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> ret:<br>        <span class="hljs-keyword">if</span> (torch.isnan(ret[k]).<span class="hljs-built_in">any</span>() <span class="hljs-keyword">or</span> torch.isinf(ret[k]).<span class="hljs-built_in">any</span>()) <span class="hljs-keyword">and</span> DEBUG:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;! [Numerical Error] <span class="hljs-subst">&#123;k&#125;</span> contains nan or inf.&quot;</span>)<br><br>    <span class="hljs-keyword">return</span> <br></code></pre></td></tr></table></figure><p>个人认为最关键的点就是上述这些，其他的相对这几项而言就更容易阅读了。</p><h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>NeRF出现后越来越多的研究者开始关注这方面的工作。对于NeRF的训练个人认为除了需要好的拍摄图片组外，对于每一条ray的采样也是很关键的。由于NeRF是将信息存储在MLP中，因此它需要在不同场景都重新训练，将信息编码进去。</p><br /><p>现在也有很多的研究是关于快速训练NeRF，以及可驱动的NeRF来让其有更好的实际应用，这些会对NeRF的实用价值有进一步的提高，期待相关研究的发展。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://arxiv.org/pdf/2003.08934.pdf">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</a></p><p><a href="https://github.com/yenchenlin/nerf-pytorch">NeRF-PyTorch</a></p><p><a href="https://www.scratchapixel.com/lessons/3d-basic-rendering/ray-tracing-generating-camera-rays/generating-camera-rays">Ray-Tracing: Generating Camera Rays</a></p>]]></content>
    
    
    <categories>
      
      <category>code reading</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NeRF</tag>
      
      <tag>code reading</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CUDA Memory Copy</title>
    <link href="/2022/06/25/cudamemcpy/"/>
    <url>/2022/06/25/cudamemcpy/</url>
    
    <content type="html"><![CDATA[<h1 id="CUDA-memcpy"><a href="#CUDA-memcpy" class="headerlink" title="CUDA memcpy"></a>CUDA memcpy</h1><blockquote><p>CUDA Memory Copy</p></blockquote><h1 id="CUDA内存拷贝"><a href="#CUDA内存拷贝" class="headerlink" title="CUDA内存拷贝"></a>CUDA内存拷贝</h1><p>在CUDA程序的内存数据拷贝中包含以下几种情况：Host2Device、Device2Host和Device2Device（Host2Host即正常程序中的copy）。在刚开始编写CUDA程序时对cudaArray和其他DeviceArray的copy存在较大的疑惑，在此对CUDA memcpy进行一个尽量详细的记录。</p><h1 id="cudaArray以及绑定纹理对象、表面对象的拷贝"><a href="#cudaArray以及绑定纹理对象、表面对象的拷贝" class="headerlink" title="cudaArray以及绑定纹理对象、表面对象的拷贝"></a>cudaArray以及绑定纹理对象、表面对象的拷贝</h1><p>以2D图像数组为例进行说明。</p><h2 id="Host2Device"><a href="#Host2Device" class="headerlink" title="Host2Device"></a>Host2Device</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-built_in">cudaMemcpy2DToArray</span>(<br>cudaArray_t dst,<br><span class="hljs-type">size_t</span> wOffset,<br><span class="hljs-type">size_t</span> hOffset,<br><span class="hljs-type">const</span> <span class="hljs-type">void</span>* src,<br><span class="hljs-type">size_t</span> spitch,<br><span class="hljs-type">size_t</span> width,<br><span class="hljs-type">size_t</span> height,<br>cudaMemcpyKind kind<br>);<br><span class="hljs-built_in">cudaMemcpy2DToArray</span>(<br>_cuda_array, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,<br>_host.data,<br>_host.cols * _host.<span class="hljs-built_in">elemSize</span>(),<br>_host_cols * _host.<span class="hljs-built_in">elemSize</span>(),<br>_host.rows,<br>cudaMemcpyHostToDevice<br>);<br></code></pre></td></tr></table></figure><p>使用函数<code>cudaMallocPitch</code>和<code>cudaMemcpy2D</code>来使用二维数组。C&#x2F;C++中二维数组内存分配是转化为一维数组，连贯紧凑，每次访问数组中的元素都必须从数组首元素开始遍历；而CUDA中分配的二维数组内存保证数组每一行首元素的地址值都按照 256 或 512 的倍数对齐，提高访问效率，但使得每行末尾元素与下一行首元素地址可能不连贯，使用指针寻址时要注意考虑尾部。</p><p><code>cudaMallocPitch</code>传入存储器指针<code>**devPtr</code>，偏移值的指针<code>*pitch</code>，数组行字节数<code>widthByte</code>，数组行数<code>height</code>。函数返回后指针指向分配的内存（每行地址对齐到 AlignByte 字节，为 256B 或 512B），偏移值指针指向的值为该行实际字节数&#x3D;sizeof(datatype) * width + alignByte - 1) &#x2F; alignByte）。</p><p><code>cudaMemcpy2D</code>传入目标存储器的指针<code>*dst</code>，目标存储器行字节数<code>dpitch</code>，源存储器指针<code>*src</code>，源存储器行字节数<code>spitch</code>，数组行字节数<code>widthByte</code>，数组行数<code>height</code>，拷贝方向 <code>kind</code>。这里要求存储器行字节数不小于数组行字节数，多出来的部分就是每行尾部空白部分。</p><p>我们在使用<code>cudaMemcpy2DToArray</code>时，第一个存放的为<code>cudaArray_t</code>的参数，个人理解为指向GPU中某一个内存地址的指针，但采用<code>cudaArray_t</code>的格式对其进行封装。在利用<code>cv::Mat</code>对二维图像进行host到device的拷贝时，一般pitch的大小和行字节数相等。</p><p>对pitch的理解，个人认为和C++中的内存对齐类似，提高了读写速率。</p><h2 id="Device2Host"><a href="#Device2Host" class="headerlink" title="Device2Host"></a>Device2Host</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-built_in">cudaMemcpy2DFromArray</span>(<br><span class="hljs-type">void</span>* dst,<br><span class="hljs-type">size_t</span> dpitch,<br>cudaArray_const_t src,<br><span class="hljs-type">size_t</span> wOffset,<br><span class="hljs-type">size_t</span> hOffset,<br><span class="hljs-type">size_t</span> width,<br><span class="hljs-type">size_t</span> height,<br>cudaMemcpyKind kind<br>);<br><span class="hljs-built_in">cudaMemcpy2DFromArray</span>(<br>_host.data,<br>_host.cols * _host.<span class="hljs-built_in">elemSize</span>(),<br>_cuda_array,<br><span class="hljs-number">0</span>, <span class="hljs-number">0</span>,<br>_host.cols * _host.<span class="hljs-built_in">elemSize</span>(),<br>_host.rows,<br>cudaMemcpyDeviceToHost<br>);<br></code></pre></td></tr></table></figure><p>Device到Host拷贝和Host拷贝到Device基本一致，仅仅是参数传入的位置进行了改变。</p><h2 id="Device2Device"><a href="#Device2Device" class="headerlink" title="Device2Device"></a>Device2Device</h2><p>一般也利用<code>cudaMemcpy2DToArray</code>的方式，将host的data地址改成GPU中指向某个分配的数据块指针即可。</p><h2 id="cudaTexture-t"><a href="#cudaTexture-t" class="headerlink" title="cudaTexture_t"></a>cudaTexture_t</h2><p>当我们获取到<code>cudaTexture_t</code>的句柄时，但并不能直接获得<code>cudaArray_t</code>的句柄，可以使用desc的方式对<code>cudaArray_t</code>数据进行获取。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs cpp">cudaResourceDesc desc;<br><span class="hljs-built_in">cudaGetTextureObjectResourceDesc</span>(&amp;desc, _cuda_texobj);<br><span class="hljs-built_in">cudaMemcpy2DFromArray</span>(<br>_host.data,<br>_host.cols * _host.<span class="hljs-built_in">elemSize</span>(),<br>desc.res.array.array,<br><span class="hljs-number">0</span>, <span class="hljs-number">0</span>,<br>_host.cols * _host.<span class="hljs-built_in">elemSize</span>(),<br>_host.rows,<br>cudaMemcpyDeviceToHost<br>);<br></code></pre></td></tr></table></figure><p>使用这种方式就能够不去获取<code>cudaArray_t</code>的句柄，通过<code>cudaTextureObject_t</code>就可以直接获取到CUDA上存储的数组并下载到Host上。</p><h1 id="cudaArray和cuda对象的拷贝"><a href="#cudaArray和cuda对象的拷贝" class="headerlink" title="cudaArray和cuda对象的拷贝"></a>cudaArray和cuda对象的拷贝</h1><p><code>cudaArray_t</code>和CUDA malloc对象之间的拷贝，也可以使用<code>cudaMemcpy2DFromArray</code>、<code>cudaMemcpy2DToArray</code>的方式来实现，同样在编写好对应指针地址、pitch大小等参数后，利用<code>cudaMemcpyDeviceToDevice</code>进行拷贝。</p><p>通常，拷贝采用Async的异步拷贝方式进行，并采用<code>cudaStreamSynchronize</code>的方式完成同步。这是因为CUDA指令基本都是异步进行，程序不会等待CUDA调用结束再进行下一步操作。但拷贝的指令若不实用Async异步方式则会让CPU进行等待，造成最终程序时长过大。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://nichijou.co/cudaRandom-memAlign/">Memory Alignment For CUDA - Fang’s Notebook</a></p>]]></content>
    
    
    <categories>
      
      <category>CUDA</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CUDA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OpenGL渲染流程</title>
    <link href="/2022/06/25/OpenGL%E6%B8%B2%E6%9F%93%E6%B5%81%E7%A8%8B/"/>
    <url>/2022/06/25/OpenGL%E6%B8%B2%E6%9F%93%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="OpenGL渲染流程"><a href="#OpenGL渲染流程" class="headerlink" title="OpenGL渲染流程"></a>OpenGL渲染流程</h1><blockquote><p>OpenGL，Open Graphics Library</p></blockquote><h1 id="渲染管线"><a href="#渲染管线" class="headerlink" title="渲染管线"></a>渲染管线</h1><p>在OpenGL中，任何事物都处于3D空间中，而屏幕和窗口却都是2D像素数组，这就导致了OpenGL大部分工作都是关于把3D坐标转变为适配你屏幕的2D像素。</p><p>渲染管线，指的是一堆原始图形数据途经一个输送管道，期间经过各种变化处理最终出现在屏幕的过程。图形渲染管线（3D坐标→2D坐标）主要可划分为：把3D坐标转换为2D坐标和把2D坐标转变为实际的有颜色的像素两个部分。</p><p>3D坐标转为2D坐标的处理过程是由OpenGL的图形渲染管线管理的。</p><h1 id="渲染管线处理流程"><a href="#渲染管线处理流程" class="headerlink" title="渲染管线处理流程"></a>渲染管线处理流程</h1><p>Vertex Data → Vertex Shader → Shape Assembly → Geometry Shader → Rasterization → Fragment Shader → Blending</p><p>灰色为绑定流程，不需要自定义编写代码。</p><p>一般而言，通过对shader的编写（Vertex Shader，.vs；Geometry Shader，.gs；Fragment Shader，.fs）来完成相应的效果呈现。</p><p>首先，以数组的形式传递3个3D坐标作为图形渲染管线的输入，用来表示一个三角形，这个数组叫做顶点数据（Vertex Data）。顶点数据是一系列顶点的集合。一个顶点（Vertex）是一个3D坐标的数据的集合。而顶点数据是用顶点属性（Vertex Attribute）表示的，它可以包含任何我们想用的数据。</p><aside>💡 OpenGL在对坐标和颜色构成进行分析渲染时候，需要人为指定数据所表示的渲染类型，如点（GL_POINTS）、线（GL_LINE_STRIP）、三角（GL_TRIANGLES）等。做出的这些提示叫做图元（Primitive），任何一个绘制指令的调用都将把图元传递给OpenGL。</aside><p>图形渲染管线的第一个部分是顶点着色器（Vertex Shader），它把一个单独的顶点作为输入。顶点着色器主要的目的是把3D坐标转为另一种3D坐标，同时顶点着色器允许我们对顶点属性进行一些基本处理。</p><p>图元装配（Primitive Assembly）阶段将顶点着色器输出的所有顶点作为输入（如果是GL_POINTS，那么就是一个顶点），并所有的点装配成指定图元的形状。</p><p>图元装配阶段的输出会传递给几何着色器（Geometry Shader）。几何着色器把图元形式的一系列顶点的集合作为输入，它可以通过产生新顶点构造出新的图元来生成其他形状。</p><p>几何着色器的输出会进行光栅化（Rasterization Stage），这里它会把图元映射为最终屏幕上相应的像素，生成供片段着色器（Fragment Shader）使用的片段（Fragment）。在片段着色器运行之前会执行裁切（Clipping）。裁切会丢弃超出视图以外的所有像素，用来提升执行效率。</p><aside>💡 OpenGL中的一个片段是OpenGL渲染一个像素所需的所有数据。</aside><p>Fragment Shader的主要目的是计算一个像素的最终颜色，这也是所有OpenGL高级效果产生的地方。通常，Fragment Shader包含3D场景的数据（如光照、阴影、光的颜色等），这些数据可以被用来计算最终像素的颜色。</p><p>在所有对应颜色值确定以后，最终的对象将会被传到最后一个阶段，我们叫做Alpha测试和混合（Blending）阶段。这个阶段检测片段的对应的深度（和模板Stencil）值，用它们来判断这个像素是其它物体的前面还是后面，决定是否应该丢弃。这个阶段也会检查Aplha值（RGBA中的A）并对物体进行Blending。所以，即使在片段着色器中计算出来了一个像素输出的颜色，在渲染多个三角形的时候最后的像素颜色也可能完全不同。</p><h1 id="OpenGL编写需要"><a href="#OpenGL编写需要" class="headerlink" title="OpenGL编写需要"></a>OpenGL编写需要</h1><p>OpenGL 首先接收用户提供的几何数据（顶点和几何图元），并且将它输入到一系列Shader中进行处理，包括:顶点着色、细分着色(它本身包含两个着色器)，以及最后的几何着色，然后它将被送入Rasterization。Rasterization负责对所有剪切（Clipping）内的图元生成Fragment，然后对每个生成的片元都执行一个Fragment Shader。</p><ul><li>Shader是OpenGL中最重要的组成，可以控制使用不同的Shader来实现所需的功能</li><li>只有Vertex Shader和Fragment Shader是必须的，细分和Geometry Shader是可选的步骤</li></ul><h1 id="数据与数据流"><a href="#数据与数据流" class="headerlink" title="数据与数据流"></a>数据与数据流</h1><ul><li>VAO，Vertex Array Object，定点数组对象，保存缓存以及顶点属性状态信息</li><li>VBO，Vertex Buffer Object，定点缓存对象，用于分配内存，保存顶点数据给图形卡使用的一种缓存对象</li><li>EBO&#x2F;IBO，Element Buffer Object&#x2F;Index Buffer Object，保存顶点索引的一种缓存对象</li></ul><p>在定义好顶点数据以后，需要在内存中存储这些顶点，VBO管理这个内存，它会在显存中储存大量顶点。使用这些缓冲对象的好处是可以一次性的发送一大批数据到显卡上，而不是每个顶点发送一次。从CPU把数据发送到显卡相对较慢，所以只要可能都要尽量一次性发送尽可能多的数据。当数据发送至显卡的内存中后，顶点着色器几乎能立即访问顶点。</p><h1 id="Vertex-Shader"><a href="#Vertex-Shader" class="headerlink" title="Vertex Shader"></a>Vertex Shader</h1><ul><li>顶点着色器对顶点实现了一种通用的可编程方法，它一般用来处理图形每个顶点变换（旋转&#x2F;平移&#x2F;投影等），顶点着色器是OpenGL中用于计算顶点属性的程序</li><li>每个顶点都会执行依次顶点着色器，执行顺序是按照存储在顶点数组的顺序依次处理（一般是逆时针）</li><li>输入数据组成<ul><li>Attributes，使用顶点数组封装每个顶点的数据，一般用于每个顶点都各不相同的变量，如顶点位置、颜色等</li><li>Uniforms，顶点着色器使用的常量数据，不能被着色器修改，一般用于对同一组顶点组成的单个3D物体中所有顶点都相同的变量，如当前光源的位置</li><li>Samplers，这个是可选的，一种特殊的Uniforms，表示顶点着色器使用的纹理</li><li>Shader Program，顶点着色器的源码或可执行文件，描述了将对顶点执行的操作。</li></ul></li><li>输出数据组成<ul><li>顶点着色器的输出数据是Varying变量，在图元光栅化阶段，这些Varying值为每个生成的片元进行计算，并将结果作为片元着色器的输入数据。从分配给每个顶点的原始Varying值来为每个片元生成一个Varying值的机制为插值</li></ul></li></ul><h1 id="Rasterization"><a href="#Rasterization" class="headerlink" title="Rasterization"></a>Rasterization</h1><ul><li>光栅化（Rasterization）是把顶点数据转换为片元的过程，具有将图转化为一个个栅格组成的图象的作用，特点是每个元素对应帧缓冲区中的一像素</li><li>光栅化其实是一种将几何图元变为二维图像的过程。该过程包含了两部分的工作。1. 决定窗口坐标中的哪些整型栅格区域被基本图元占用；2. 分配一个颜色值和一个深度值到各个区域。光栅化过程产生的是片元</li><li>把物体的数学描述以及与物体相关的颜色信息转换为屏幕上用于对应位置的像素及用于填充像素的颜色，这个过程称为光栅化，这是一个将模拟信号转化为离散信号的过程</li></ul><p>简而言之，光栅化阶段绘制对应的图元（点、线、三角形），将图元转化为一组二维数组的过程，然后传递给Fragment Shader处理，数组中每一个数就是pixel</p><h1 id="Fragment-Shader"><a href="#Fragment-Shader" class="headerlink" title="Fragment Shader"></a>Fragment Shader</h1><p>Fragment Shader主要是对光栅化处理后生成的片元逐个进行 并行处理。接收Vertex Shader输出的值，需要传入的数据，以及它经过变换矩阵后输出值存储位置。</p><ul><li>输入数据组成<ul><li>Shader Program，描述片元所执行的片元着色器程序源代码输入变量</li><li>Rasterization对Vertex Shader插值后的输出值</li><li>Uniform，Fragment Shader使用的不变的数据</li><li>Sampler，代表Fragment Shader所用纹理的一种特殊的统一变量类型</li></ul></li><li>输出数据组成<ul><li>Fragment Color</li></ul></li></ul><h1 id="Blending"><a href="#Blending" class="headerlink" title="Blending"></a>Blending</h1><p>Blend 混合是将源色和目标色以某种方式混合生成特效的技术，混合常用来绘制有透明度的物体。</p><p>在混合中起关键作用的$\alpha$值实际上是将源色和目标色按给定比率进行混合，以达到不同程度的透明。$\alpha$值为0则完全透明，$\alpha$值为1则完全不透明。混合操作只能在RGBA模式下进行，颜色索引模式下无法指定$\alpha$值。</p><p>物体的绘制顺序会影响到OpenGL的混合处理。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.jianshu.com/p/0992fecb5107">OpenGL-渲染流程</a></p><p><a href="https://www.cnblogs.com/bigfeng/p/5068715.html">BigFengFeng</a></p>]]></content>
    
    
    <categories>
      
      <category>render</category>
      
    </categories>
    
    
    <tags>
      
      <tag>opengl</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Occypancy Networks</title>
    <link href="/2022/06/25/OccupancyNetworks/"/>
    <url>/2022/06/25/OccupancyNetworks/</url>
    
    <content type="html"><![CDATA[<h1 id="Occupancy-Networks"><a href="#Occupancy-Networks" class="headerlink" title="Occupancy Networks"></a>Occupancy Networks</h1><blockquote><p>Occupancy Networks: Learning 3D Reconstruction in Function Space</p></blockquote><h1 id="Target"><a href="#Target" class="headerlink" title="Target"></a>Target</h1><p>将3D表面使用深度神经网络表示称连续的分类（二分类）问题。</p><h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h1><p>在空间体素中，需要重建的物体的占有率并不是离散的3D点位置，而是每一个可能的3D点$p\in \mathbb{R}^3$，均有推论函数（occupancy function）$o:\mathbb{R}^3 \to {0,1}$</p><p>利用神经网络估计推论函数，空间体素每一个点都能够预测一个0到1的占有率。实际上，网络就是进行一个二分类的判断，对于空间体素的每一个点都能够生成其是否为物体表面（是否在物体内部）的概率。</p><p>因此在使用网络进行重建之前，对于输入和输出进行了对应。对于给定的观测输入$x\in \mathcal X$，对于网络的输出$p\in \mathbb R^3$能够使用$(p,x)\in \mathbb R^3 \times \mathcal X$来表示。即对于一个参数化神经网络$f_\theta(\cdot)$，对于给定的pair $(p,x)$能够有</p><p>$$<br>f_\theta:\mathcal R^3 \times \mathbb X \to [0,1]<br>$$</p><p>其中，输入输出都为实数。</p><h2 id="训练策略"><a href="#训练策略" class="headerlink" title="训练策略"></a>训练策略</h2><p>在对象的三维边界体中随机采样点，对于第i个样本，采样K个点，然后评估这些位置的小批量损失为</p><p>$$<br>\mathcal L_\mathcal B(\theta)&#x3D;\frac{1}{|\mathcal B|}\sum_{i&#x3D;1}^{|\mathcal B|}\sum_{j&#x3D;1}^K\mathcal L(f_\theta(p_{ij},x_i),o_{ij})<br>$$</p><p>$x_i$是batch B中的第i个观测值，$o_{ij}\equiv o(p_{ij})$，是点云的真实位置，$\mathcal L(\cdot,\cdot)$是交叉熵。</p><p>3D表征能够学习到概率隐变量模型，于是论文介绍了一个encoder网络$g_\psi(\cdot)$，使用位置$p_{ij}$和占有$o_{ij}$作为输入，最终预测出均值$\mu_\psi$和标准差$\sigma_\psi$，其满足高斯分布$q_\psi(z|(p_{ij},o_{ij})<em>{j&#x3D;1:K}$，且隐空间$z\in \mathbb R^L$。作者利用生成模型$p((o</em>{ij})<em>{j&#x3D;1:K}|(p</em>{ij})_{j&#x3D;1:K})$负对数似然来优化下边界</p><p>$$<br>\mathcal L_\mathcal B^{gen}(\theta,\psi)&#x3D;\frac{1}{|\mathcal B|}\sum_{i&#x3D;1}^{|\mathcal B|}[\sum_{j&#x3D;1}^K\mathcal L(f_\theta(p_{ij},z_i),o_{ij})+KL(q_\psi(z|(p_{ij},o_{ij})_{j&#x3D;1:K})||p_0(z))]<br>$$</p><p>KL即KL散度，$p_0(z)$为隐变量$z_i$的先验分布，$z_i$是通过$q_\psi(z|(p_{ij},o_{ij})_{j&#x3D;1:K})$采样得到。</p><h2 id="推理阶段"><a href="#推理阶段" class="headerlink" title="推理阶段"></a>推理阶段</h2><p>论文为了根据训练出的Occupancy Network在新的观测下得到的结果提取出等值面，提出MISE（Multiresolution IsoSurface Extraction，多分辨率等值面提取）。</p><p><img src="/./2022/06/25/OccupancyNetworks/OccupancyNet.png" alt="Occupency Network"></p><p>首先在给定的分辨率上标记所有已经被评估为被占据（红）或未被占据（青）的点。然后确定所有的体素已经占领和未占领的角落，并标记（淡红），细分为4个亚体素。接下来，评估所有由细分引入的新网格点（空）。重复前两个步骤，直到达到所需的输出分辨率。最后使用Marching Cubes算法提取网格，利用一阶和二阶梯度信息对输出网格进行简化和细化。</p><h1 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h1><p>在论文给出的代码中，decoder部分最后一层为Conv1d而没有衔接Sigmoid&#x2F;Softmax层，在loss函数计算的时候，使用BCE_with_logits来计算。同时，将输出的logits生成dist.Bernoulli分布，用KL散度与先验模型进行损失计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DecoderCBatchNorm</span>(nn.Module):<br>    <span class="hljs-string">&#x27;&#x27;&#x27; Decoder with conditional batch normalization (CBN) class.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        dim (int): input dimension</span><br><span class="hljs-string">        z_dim (int): dimension of latent code z</span><br><span class="hljs-string">        c_dim (int): dimension of latent conditioned code c</span><br><span class="hljs-string">        hidden_size (int): hidden size of Decoder network</span><br><span class="hljs-string">        leaky (bool): whether to use leaky ReLUs</span><br><span class="hljs-string">        legacy (bool): whether to use the legacy structure</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dim=<span class="hljs-number">3</span>, z_dim=<span class="hljs-number">128</span>, c_dim=<span class="hljs-number">128</span>,</span><br><span class="hljs-params">                 hidden_size=<span class="hljs-number">256</span>, leaky=<span class="hljs-literal">False</span>, legacy=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.z_dim = z_dim<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> z_dim == <span class="hljs-number">0</span>:<br>            self.fc_z = nn.Linear(z_dim, hidden_size)<br><br>        self.fc_p = nn.Conv1d(dim, hidden_size, <span class="hljs-number">1</span>)<br>        self.block0 = CResnetBlockConv1d(c_dim, hidden_size, legacy=legacy)<br>        self.block1 = CResnetBlockConv1d(c_dim, hidden_size, legacy=legacy)<br>        self.block2 = CResnetBlockConv1d(c_dim, hidden_size, legacy=legacy)<br>        self.block3 = CResnetBlockConv1d(c_dim, hidden_size, legacy=legacy)<br>        self.block4 = CResnetBlockConv1d(c_dim, hidden_size, legacy=legacy)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> legacy:<br>            self.bn = CBatchNorm1d(c_dim, hidden_size)<br>        <span class="hljs-keyword">else</span>:<br>            self.bn = CBatchNorm1d_legacy(c_dim, hidden_size)<br><br>        self.fc_out = nn.Conv1d(hidden_size, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> leaky:<br>            self.actvn = F.relu<br>        <span class="hljs-keyword">else</span>:<br>            self.actvn = <span class="hljs-keyword">lambda</span> x: F.leaky_relu(x, <span class="hljs-number">0.2</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, p, z, c, **kwargs</span>):<br>        p = p.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        batch_size, D, T = p.size()<br>        net = self.fc_p(p)<br><br>        <span class="hljs-keyword">if</span> self.z_dim != <span class="hljs-number">0</span>:<br>            net_z = self.fc_z(z).unsqueeze(<span class="hljs-number">2</span>)<br>            net = net + net_z<br><br>        net = self.block0(net, c)<br>        net = self.block1(net, c)<br>        net = self.block2(net, c)<br>        net = self.block3(net, c)<br>        net = self.block4(net, c)<br><br>        out = self.fc_out(self.actvn(self.bn(net, c)))<br>        out = out.squeeze(<span class="hljs-number">1</span>)<br><br>        <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_loss</span>(<span class="hljs-params">self, data</span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27; Computes the loss.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            data (dict): data dictionary</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        device = self.device<br>        p = data.get(<span class="hljs-string">&#x27;points&#x27;</span>).to(device)<br>        occ = data.get(<span class="hljs-string">&#x27;points.occ&#x27;</span>).to(device)<br>        inputs = data.get(<span class="hljs-string">&#x27;inputs&#x27;</span>, torch.empty(p.size(<span class="hljs-number">0</span>), <span class="hljs-number">0</span>)).to(device)<br><br>        kwargs = &#123;&#125;<br><br>        c = self.model.encode_inputs(inputs)<br>        q_z = self.model.infer_z(p, occ, c, **kwargs)<br>        z = q_z.rsample()<br><br>        <span class="hljs-comment"># KL-divergence</span><br>        kl = dist.kl_divergence(q_z, self.model.p0_z).<span class="hljs-built_in">sum</span>(dim=-<span class="hljs-number">1</span>)<br>        loss = kl.mean()<br><br>        <span class="hljs-comment"># General points</span><br>        logits = self.model.decode(p, z, c, **kwargs).logits<br>        loss_i = F.binary_cross_entropy_with_logits(<br>            logits, occ, reduction=<span class="hljs-string">&#x27;none&#x27;</span>)<br>        loss = loss + loss_i.<span class="hljs-built_in">sum</span>(-<span class="hljs-number">1</span>).mean()<br><br>        <span class="hljs-keyword">return</span> loss<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>paper reading</category>
      
    </categories>
    
    
    <tags>
      
      <tag>volume reconstruction</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Marching Cubes</title>
    <link href="/2022/06/25/MarchingCubes/"/>
    <url>/2022/06/25/MarchingCubes/</url>
    
    <content type="html"><![CDATA[<h1 id="Marching-Cubes"><a href="#Marching-Cubes" class="headerlink" title="Marching Cubes"></a>Marching Cubes</h1><p>Marching Cubes算法是三维离散数据场中提取等值面的经典算法，其主要应用于医学领域的可视化场景，例如CT扫描和MRI扫描的3D重建等值面，即空间中所有具有某个相同值的点的集合，可以类比为地形图里的等高线。</p><p>${ (x,y,z)|f(x,y,z)&#x3D;c, c\in C  }$</p><h1 id="算法思想"><a href="#算法思想" class="headerlink" title="算法思想"></a>算法思想</h1><ul><li>基本假设：沿六面体边的数据场呈连续性变化。如果一条边的两个顶点分别大于或小于等值面的值，则在该条边上有且仅有一点是这条边与等值面的交点。</li><li>逐个处理数据场中的立方体（体素voxel），分离出与等值面相交的立方体，采用插值计算出等值面与立方体边的交点。根据立方体每一顶点与等值面的相对位置，将等值面与立方体边的交点按一定方式连接生成等值面，作为等值面在该立方体内的一个逼近表示</li><li>直观地说，就是用许多小正方体去对空间进行切分，然后用小正方体内部的平面来近似表示当前的等值面。小正方体的数量越多，逼近的效果越好，计算需要的资源就越多</li></ul><h1 id="部分细节"><a href="#部分细节" class="headerlink" title="部分细节"></a>部分细节</h1><ul><li>小正方体内部的平面可以小立方体和等值面的相交情况来确定</li></ul><p><img src="/./2022/06/25/MarchingCubes/Judge.png" alt="Judge Cube"></p><ul><li>对于每个小正方体来说，每个顶点两种情况（大于或小于）当前等值面的值，8个顶点共256种情况，考虑到旋转对称性，从新分类后可得15种基本模式</li></ul><p><img src="/./2022/06/25/MarchingCubes/MarchingCubeCases.png" alt="Marching Cube Cases"></p><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><ol><li>将原始数据经过预处理之后，读入指定的数组中</li><li>从网格数据体中提取一个单元体，成为当前单元体，同时获取该单元体的所有信息，例如8个顶点的值、坐标位置等</li><li>将当前单元体8个顶点的函数值与给定等值面值C进行比较，得到该单元体的状态表（edgeTable、triTable）</li><li>根据当前单元体的状态表索引，找出与等值面相交的单元体棱边，并采用线性插值的方法，计算出各个交点的位置坐标</li><li>利用中心差分法，求出当前单元体8个顶点的法向量，在采用线性插值的方法，得到三角面片各个顶点的法向</li><li>根据各个三角面片顶点的坐标，顶点法向量进行等值面图象的绘制</li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://zhuanlan.zhihu.com/p/48022195">Marching Cubes算法理解</a></p><p><a href="https://tandy123.github.io/2017/03/02/Marching-Cubes/">【算法】Marching Cubes算法理解</a></p>]]></content>
    
    
    <categories>
      
      <category>3D Basic</category>
      
    </categories>
    
    
    <tags>
      
      <tag>volume</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CUDA极简入门</title>
    <link href="/2022/06/25/CUDA%E6%9E%81%E7%AE%80%E5%85%A5%E9%97%A8/"/>
    <url>/2022/06/25/CUDA%E6%9E%81%E7%AE%80%E5%85%A5%E9%97%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="CUDA极简入门"><a href="#CUDA极简入门" class="headerlink" title="CUDA极简入门"></a>CUDA极简入门</h1><blockquote><p>CUDA，Compute Unified Device Architecture</p></blockquote><aside>💡 这是一份极简的CUDA入门编程指南，确保至少了解C/C++</aside><h1 id="典型流程"><a href="#典型流程" class="headerlink" title="典型流程"></a>典型流程</h1><ol><li>分配host内存，并进行数据初始化</li><li>分配device内存，并从host将数据拷贝到device上</li><li>调用CUDA的核函数在device上完成指定的运算</li><li>将device上的运算结果拷贝到host上</li><li>释放device和host上分配的内存</li></ol><h1 id="CUDA数组、对象和资源的使用"><a href="#CUDA数组、对象和资源的使用" class="headerlink" title="CUDA数组、对象和资源的使用"></a>CUDA数组、对象和资源的使用</h1><p>一般使用<code>cudaArray_t</code>、<code>cudaTextureObj_t</code>和<code>cudaSurfaceObj_t</code>三种，并且常将同一个变量的三者（or less，但至少使用）形式进行绑定。<code>cudaChannelFormatDesc</code>和<code>cudaCreateChannelDesc</code>的使用为<code>cudaArray_t</code>、<code>cudaTextureObj_t</code>和<code>cudaSurfaceObj_t</code>进行格式创建描述，使用<code>MallocCudaArray</code>对相关Resource进行显存分配。同时，如果需要将GPU中的数据与CPU中的数据进行同步或交换，采用<code>cudaMemcpy2DToArray</code>和<code>cudaMemcpy2DFromArray</code>，完成GPU和CPU的数据同步。</p><h1 id="Error定位"><a href="#Error定位" class="headerlink" title="Error定位"></a>Error定位</h1><p>由于CUDA是跑在GPU上的程序，所有的变量分配也是存放在GPU中，因此不能够和CPU调试一样有详细的输出或堆栈调用。但我们在编写时总可能会遇到在某一些代码段出错的情况，但CUDA只会抛出异常，并不会定位到代码段，因此需要进行GPU代码模块的Error定位。</p><p>一般而言，我们在完成GPU相应的代码模块功能后，可以使用<code>cudaGetLastError</code>和<code>cudaStreamSynchtonize</code>来定位可能出现的CUDA Error。这两个指令可以追踪CUDA中遇到的Error，在程序中可以使用多个追踪。在CUDA模块运行出错后，程序会根据Error追踪指令提示到出错CUDA模块后的第一个<code>cudaGetLastError</code>，用此方式可以快速定位到编写的哪一个模块在最终运行时出现错误。</p><p>实际上，所有的CUDA API都会返回一个CUDA状态值，但我们一般不需要将所有的CUDA命令都进行出错判断。</p><h1 id="From-simple-example"><a href="#From-simple-example" class="headerlink" title="From simple example"></a>From simple example</h1><p>示例代码功能为利用CUDA实现灰度图片对应像素相加。（当然我还没跑过，虽然是我写的）</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;opencv2/core.hpp&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;opencv2/imgcodecs.hpp&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cuda_runtime.h&gt;</span></span><br><br><span class="hljs-comment">// __global__ declare</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">add</span><span class="hljs-params">(cudaTextureObject_t* img1,cudaTextureObject_t* img2,cudaSurfaceObject_t* img3, <span class="hljs-type">int</span> imgH, <span class="hljs-type">int</span> imhW)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-type">int</span> pidx = blockIdx.x * blockDim.x + threadIdx.x;<br><span class="hljs-type">int</span> pidy = blockIdx.y * blockDim.y + threadIdx.y;<br><span class="hljs-keyword">if</span>((pidx&lt;imgW)&amp;&amp;(pidy&lt;imgH))&#123;<br>uchar v1 = <span class="hljs-built_in">tex2D</span>&lt;uchar&gt;(img1, pidx, pidy);<br>uchar v2 = <span class="hljs-built_in">tex2D</span>&lt;uchar&gt;(img2, pidx, pidy);<br>uchar v3 = v1 + v2;<br><span class="hljs-built_in">surf2Dwrite</span>(v3, img3, <span class="hljs-built_in">sizeof</span>(uchar)*pidx, pidy);<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">Upload</span><span class="hljs-params">(cv::Mat &amp;_img_host, cudaArray_t &amp;_cuda_array)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-built_in">cudaSafeCall</span>(<span class="hljs-built_in">cudaMemcpy2DToArray</span>(<br>_cuda_array, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,<br>_img_host.data,<br>_img_host.cols * _img_host.<span class="hljs-built_in">elemSize</span>(),<br>_img_host.cols * _img_host.<span class="hljs-built_in">elemSize</span>(),<br>_img_host.rows,<br>cudaMemcpyHostToDevice));<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">Download</span><span class="hljs-params">(cudaArray_t &amp;_cuda_array, cv::Mat &amp;_img_host)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-built_in">cudaSafeCall</span>(<span class="hljs-built_in">cudaMemcpy2DFromArray</span>(<br>_img_host.data,<br>_img_host.cols * _img_host.<span class="hljs-built_in">elemSize</span>(),<br>_cuda_array,<br><span class="hljs-number">0</span>, <span class="hljs-number">0</span>,<br>_img_host.cols * _img_host.<span class="hljs-built_in">elemSize</span>(),<br>_img_host.rows,<br>cudaMemcpyDeviceToHost));<br>&#125;<br><br><span class="hljs-keyword">auto</span> CreateCudaTextureAndSurfaceObject = [](<br>cudaTextureObject_t &amp;_tex,<br>cudaSurfaceObject_t &amp;_surf,<br>cudaArray_t &amp;_array,<br>cudaResourceDesc &amp;_res_desc,<br>cudaTextureDesc &amp;_tex_desc)<br>&#123;<br>_res_desc.res.array.array = _array;<br><span class="hljs-built_in">cudaSafeCall</span>(<span class="hljs-built_in">cudaCreateTextureObject</span>(&amp;_tex, &amp;_res_desc, &amp;_tex_desc, <span class="hljs-number">0</span>));<br><span class="hljs-built_in">cudaSafeCall</span>(<span class="hljs-built_in">cudaCreateSurfaceObject</span>(&amp;_surf, &amp;_res_desc));<br>&#125;;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-comment">// Host array init</span><br>    cv::Mat host_a, host_b, host_c;<br><span class="hljs-type">int</span> width = <span class="hljs-number">512</span>, height = <span class="hljs-number">512</span>;<br>host_a.<span class="hljs-built_in">create</span>(width, height, CV_8CU1);<br>host_b.<span class="hljs-built_in">create</span>(width, height, CV_8CU1);<br>host_c.<span class="hljs-built_in">create</span>(width, height, CV_8CU1);<br><br><span class="hljs-comment">// Device array, surface, texture init</span><br>cudaArray_t dev_a_cuarray;<br>cudaArray_t dev_b_cuarray;<br>cudaArray_t dev_c_cuarray;<br><br>cudaTextureObject_t dev_a_cutex;<br>cudaTextureObject_t dev_b_cutex;<br>cudaTextureObject_t dev_c_cutex;<br><br>cudaSurfaceObject_t dev_a_cusurf;<br>cudaSurfaceObject_t dev_b_cusurf;<br>cudaSurfaceObject_t dev_c_cusurf;<br><br>cudaChannelFormatDesc dev_a_desc = <span class="hljs-built_in">cudaCreateChannelDesc</span>&lt;uchar&gt;();<br>cudaChannelFormatDesc dev_b_desc = <span class="hljs-built_in">cudaCreateChannelDesc</span>&lt;uchar&gt;();<br>cudaChannelFormatDesc dev_c_desc = <span class="hljs-built_in">cudaCreateChannelDesc</span>&lt;uchar&gt;();<br><br><span class="hljs-built_in">cudaMallocArray</span>(&amp;dev_a_cuarray, &amp;dev_a_desc, width, height, cudaArraySurfaceLoadStore);<br><span class="hljs-built_in">cudaMallocArray</span>(&amp;dev_b_cuarray, &amp;dev_b_desc, width, height, cudaArraySurfaceLoadStore);<br><span class="hljs-built_in">cudaMallocArray</span>(&amp;dev_c_cuarray, &amp;dev_c_desc, width, height, cudaArraySurfaceLoadStore);<br><br>cudaTextureDesc tex_desc;<br><span class="hljs-built_in">memset</span>(&amp;tex_desc, <span class="hljs-number">0</span>, <span class="hljs-built_in">sizeof</span>(cudaTextureDesc));<br>tex_desc.addressMode[<span class="hljs-number">0</span>] = cudaAddressModeBorder;<br>tex_desc.addressMode[<span class="hljs-number">1</span>] = cudaAddressModeBorder;<br>tex_desc.filterMode = cudaFilterModePoint;<br>tex_desc.readMode = cudaReadModeElementType;<br>tex_desc.normalizedCoords = <span class="hljs-number">0</span>;<br><br>cudaResourceDesc res_desc;<br><span class="hljs-built_in">memset</span>(&amp;res_desc, <span class="hljs-number">0</span>, <span class="hljs-built_in">sizeof</span>(cudaResourceDesc));<br>res_desc.resType = cudaResourceTypeArray;<br><br><span class="hljs-built_in">CreateCudaTextureAndSurfaceObject</span>(dev_a_cutex, dev_a_cusurf, dev_a_cuarray, res_desc, tex_desc);<br><span class="hljs-built_in">CreateCudaTextureAndSurfaceObject</span>(dev_b_cutex, dev_b_cusurf, dev_b_cuarray, res_desc, tex_desc);<br><span class="hljs-built_in">CreateCudaTextureAndSurfaceObject</span>(dev_c_cutex, dev_c_cusurf, dev_c_cuarray, res_desc, tex_desc);<br><br><span class="hljs-comment">// Upload data from CPU to GPU</span><br><span class="hljs-built_in">Upload</span>(host_a, dev_a_cuarray);<br><span class="hljs-built_in">Upload</span>(host_b, dev_b_cuarray);<br><span class="hljs-built_in">Upload</span>(host_c, dev_c_cuarray);<br><br><span class="hljs-comment">// Block and Grid init</span><br><span class="hljs-function">dim3 <span class="hljs-title">block</span><span class="hljs-params">(<span class="hljs-number">16</span>, <span class="hljs-number">16</span>)</span></span>;<br><span class="hljs-function">dim3 <span class="hljs-title">grid</span><span class="hljs-params">(width/block.x, height/block.y)</span></span>;<br>    add&lt;&lt;&lt;grid, block&gt;&gt;&gt;(dev_a_cutex, dev_b_cutex, dev_c_cusurf, height, width);<br><br><span class="hljs-built_in">cudaSafeCall</span>(<span class="hljs-built_in">cudaGetLastError</span>());<br><span class="hljs-built_in">cudaSafeCall</span>(<span class="hljs-built_in">cudaDeviceSynchronize</span>());<br><br><span class="hljs-comment">// Download data from GPU to CPU</span><br><span class="hljs-built_in">Download</span>(dev_c_cuarray, host_c);<br><br><span class="hljs-comment">// Destroy CUDA resource</span><br><span class="hljs-built_in">cudaDestroyTextureObject</span>(dev_a_cutex);<br><span class="hljs-built_in">cudaDestroyTextureObject</span>(dev_b_cutex);<br><span class="hljs-built_in">cudaDestroyTextureObject</span>(dev_c_cutex);<br><br><span class="hljs-built_in">cudaDestroySurfaceObject</span>(dev_a_cusurf);<br><span class="hljs-built_in">cudaDestroySurfaceObject</span>(dev_b_cusurf);<br><span class="hljs-built_in">cudaDestroySurfaceObject</span>(dev_c_cusurf);<br><br><span class="hljs-built_in">cudaFreeArray</span>(dev_a_cuarray);<br><span class="hljs-built_in">cudaFreeArray</span>(dev_b_cuarray);<br><span class="hljs-built_in">cudaFreeArray</span>(dev_c_cuarray);<br><br><span class="hljs-comment">// Release cv::Mat</span><br>host_a.<span class="hljs-built_in">release</span>();<br>host_b.<span class="hljs-built_in">release</span>();<br>host_c.<span class="hljs-built_in">release</span>();<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h1 id="Grid-and-Block"><a href="#Grid-and-Block" class="headerlink" title="Grid and Block"></a>Grid and Block</h1><p>Grid和Block是CUDA编程中最重要的概念之一，以下图[Reference 1]为例，代表一个CUDA核函数kernel调用的Grid和Block图解。</p><p><code>blockDim.x</code>是每一个Block内部的x方向的维度，图中5，即每行5个线程。<code>blockDim.y</code>是Block内部的y方向的维度，这里是3，即每列3个线程。<code>blockIdx.x</code>是Block在grid中x方向的位置，图中放大的Block是Grid中的2，即为Grid中x方向的第2个。<code>blockIdx.y</code>是Block在grid中y向的位置，图中放大的Block是Grid中的2，即为Grid中y方向的第2个。blockIdx中的Idx是表示index的缩写，而不是表示x方向的ID。</p><p><img src="/./2022/06/25/CUDA%E6%9E%81%E7%AE%80%E5%85%A5%E9%97%A8/GridAndBlock.png" alt="Grid And Block"></p><p>在CUDA kernel的使用时，需要传入&lt;&lt;&lt;grid, block&gt;&gt;&gt;参数。</p><h1 id="CUDA-C-Basic"><a href="#CUDA-C-Basic" class="headerlink" title="CUDA C++ Basic"></a>CUDA C++ Basic</h1><aside>💡 在这一篇指南中，我们将依据上述Sample简要介绍一些基础的Type和Function</aside><ul><li><code>cudaCreateChannelDesc</code>，创建cuda的通道描述符，在使用<code>cudaMallocArray</code>的时候需要将描述符一同传入进行<code>cudaArray</code>的内存申请。使用<code>cudaChannelFormatDesc dev_a_desc = cudaCreateChannelDesc&lt;uchar&gt;();</code>的使用方式生成通道描述符</li><li><code>cudaArray_t, cudaSurfaceObject_t, cudaTextureObject_t</code>为cuda中常用的三种数据类型格式，分别为数组类型、表面对象和纹理对象。表面对象和纹理对象通常会绑定到cuda数组<ol><li>若将<code>cudaArray_t, cudaSurfaceObject_t, cudaTextureObject_t</code>利用<code>CreateCudaTextureAndSurfaceObject</code>的方式进行绑定，实际上cuda数组、表面对象和纹理对象都绑定在同一个显存区间中，仅是对象读写方式不同</li><li><code>cudaArray_t</code>为cuda数组对象，是最重要的一个对象，在完成cuda数组对象的申请和分配后，才可以将相应的纹理对象、表面对象等绑定到数组对象中，并且使用cuda数组对象能更方便地与Host对象进行数据同步</li><li><code>cudaTextureObject_t</code>可读不可写，其读取速度可能比<code>cudaSurfaceObject_t</code>的速度快（未证实）。纹理对象会自动实现插值，即若取值为uv中没有恰好落在某个像素的时候，纹理对象会自动插值生成对应值</li><li><code>cudaSurfaceObject_t</code>可写，在对<code>cudaArray_t</code>需要进行更改的情况下一般都会将数组对象和表面对象进行绑定，并通过cuda kernel中<code>surf2Dwrite</code>函数将更新后的数值写入表面对象中，由于表面对象和数组对象是同一个绑定，因此cuda数组完成了更新</li></ol></li><li><code>cudaMallocArray</code>是将数组对象在GPU中分配对应显存空间的函数，类似于CPU中分配内存的函数<code>Malloc</code>。但在对cuda对象进行显存分配的过程中，除了cuda数组对象、长宽和类型外，使用通道描述符来完成cuda数组对象类型的传入，如<code>uchar, float4</code>等</li><li><code>cudaCreateTextureObject, cudaCreateSurfaceObject</code>是将cuda数组对象与表面对象和纹理对象进行绑定的函数，将表面对象和纹理对象与cuda数组对象绑定后就能够使用相应对象完成后续操作</li><li><code>cudaDestroyTextureObject, cudaDestroySurfaceObject</code>是将表面对象和纹理对象进行销毁</li><li><code>cudaFreeArray</code>是将cuda数组对象申请的GPU显存进行释放</li><li><code>cudaMemcpy2DToArray, cudaMemcpy2DFromArray</code>分别是将Host对象拷贝到Device和Device对象拷贝到Host</li><li><code>surf2Dwrite</code>是将对应2D点的值写入到<code>cudaSurfaceObject_t</code>的表面对象中，cuda数组对象与表面对象绑定后并创建格式为<code>cudaArraySurfaceLoadStore</code>后可以使用此方式进行更改，同样的表面对象读取可以使用<code>surf2Dread</code>的方式完成</li><li><code>cudaSafeCall</code>，检查cuda函数返回值是否正确的判定宏</li></ul><h1 id="CUDA核函数"><a href="#CUDA核函数" class="headerlink" title="CUDA核函数"></a>CUDA核函数</h1><p>在CUDA C++的<code>.cu</code>文件中，函数前缀分为<code>__host__, __global__, __device__</code>三种</p><ul><li><code>__host__</code>为Host调用的函数，此类型函数无法被GPU调用，无法被<code>__global__</code>和<code>__device__</code>调用。若函数未表明前缀，默认为此前缀类型</li><li><code>__global__</code>前缀代表此函数为核函数，可以在Host中调用，被Device执行，并且此函数可以调用<code>__device__</code>前缀的函数，函数使用严格按照&lt;&lt;&lt;grid, block&gt;&gt;&gt;的方式进行，函数返回类型必须是<code>void</code>，不支持可变参数参数，不能成为类成员函数。用<code>__global__</code>定义的kernel是异步的，这意味着Host不会等待Kernel执行完就执行下一步</li><li><code>__device__</code>前缀表明此函数是在GPU中运行的，无法直接被CPU函数调用</li><li><code>__host__</code>和<code>__device__</code>可以同时使用</li></ul><p>由于CUDA核函数中有不同的线程，因此需要传入grid和block参数给CUDA核函数进行使用。在CUDA核函数编写的时候，需要先计算出当前所在的线程idx，然后在参与计算。一个线程块上的线程是放在同一个流式多处理器（Streaming Multiprocessor，SM）上的，但是单个SM的资源有限，这导致线程块中的线程数是有限制的，现在的GPU线程块可支持的线程数可达1024个。当我们要知道一个线程在blcok中的全局ID，就必须还要知道block的组织结构，这是通过线程的内置变量blockDim来获得。它获取线程块各个维度的大小。对于一个2-dim的block$(D_x,D_y)$，线程$(x,y)$的ID值为$x+y\cdot D_x$，如果是3-dim的block$(D_x,D_y,D_z)$，线程$(x,y,z)$的ID值为$x+y\cdot D_x+z\cdot D_x\cdot D_y$。另外线程还有内置变量gridDim，用于获得网格块各个维度的大小。</p><p>如在2-dim的核函数中，一般有</p><p>$$<br>int \ \ pidx &#x3D;  blockIdx.x * blockDim.x + threadIdx.x \ int \ \ pidy &#x3D;  blockIdx.y * blockDim.y + threadIdx.y<br>$$</p><p>CUDA的内存模型，如下图所示。可以看到，每个线程有自己的私有本地内存（Local Memory），而每个线程块有包含共享内存（Shared Memory）,可以被线程块中所有线程共享，其生命周期与线程块一致。此外，所有的线程都可以访问全局内存（Global Memory）。还可以访问一些只读内存块：常量内存（Constant Memory）和纹理内存（Texture Memory）。</p><p><img src="/./2022/06/25/CUDA%E6%9E%81%E7%AE%80%E5%85%A5%E9%97%A8/CUDAMemModel.png" alt="CUDA Memory Model"></p><p>一个SM的基本执行单元是包含32个线程的线程束，因此block大小一般设置为32的倍数。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://zhuanlan.zhihu.com/p/34587739">CUDA编程入门极简教程</a></p><p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">Programming Guide :: CUDA Toolkit Documentation</a></p>]]></content>
    
    
    <categories>
      
      <category>CUDA</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CUDA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>苍之彼方的四重奏</title>
    <link href="/2022/06/25/%E8%8B%8D%E4%B9%8B%E5%BD%BC%E6%96%B9%E7%9A%84%E5%9B%9B%E9%87%8D%E5%A5%8F/"/>
    <url>/2022/06/25/%E8%8B%8D%E4%B9%8B%E5%BD%BC%E6%96%B9%E7%9A%84%E5%9B%9B%E9%87%8D%E5%A5%8F/</url>
    
    <content type="html"><![CDATA[<h1 id="苍之彼方的四重奏"><a href="#苍之彼方的四重奏" class="headerlink" title="苍之彼方的四重奏"></a>苍之彼方的四重奏</h1><blockquote><p>恐惧、逃避强者人之常情，但奋力一搏至少能收获沿途风景。</p></blockquote><h1 id="游戏背景"><a href="#游戏背景" class="headerlink" title="游戏背景"></a>游戏背景</h1><ul><li>人们发现了反重力子，并用此制作了能让人们在空中飞行的飞行鞋</li><li>Flying Circus（FC），使用飞行鞋进行的竞技运动</li><li>运动鞋分为竞技型和普通型，竞技型又分为速度型、战斗型和全能型，可以通过调参来改变竞技型运动鞋的模式</li><li>在久奈岛，使用飞行鞋出行较为普遍</li></ul><hr><h1 id="故事背景"><a href="#故事背景" class="headerlink" title="故事背景"></a>故事背景</h1><p>曾经的FC天才少年<strong>日向晶也</strong>（男主）在一次重大挫折后，选择结束自己FC竞技生涯。但突然有一天他遇到了一名转校生<strong>仓科明日香</strong>，为了避免迟到晶也带着明日香使用飞行鞋飞到了学校，明日香因此对飞行鞋产生了浓厚的兴趣。老师<strong>各务葵</strong>发现晶也竟然采用飞行鞋来上学，于是教唆晶也指导明日香使用飞行鞋。</p><p>在教学的过程中偶然碰见有人进行FC对决，明日香开始对FC产生极大的兴趣。机缘巧合下，晶也主开始执教学校FC部，不再以天才少年而是以教练的身份开始了新的FC生涯展开的故事。</p><hr><h1 id="剧情感受"><a href="#剧情感受" class="headerlink" title="剧情感受"></a>剧情感受</h1><p><em><strong>以下内容包含剧透</strong></em></p><p>各务葵老师不仅仅是现在晶也的班级教师，而且是FC领域非常知名的一位人物，具有高超的技巧和出色的运动能力，她看中了晶也出色的运动能力和FC天赋，并且在晶也对FC有浓厚兴趣的同时指导他成为了同年龄中非常出色的一名FC选手。但在某一天，晶也在沙滩边遇到了一名和他年龄差不多大的人，在数次的FC飞行后晶也发现这个人有着非常杰出的能力，自己长期所学都快赶不上这人的天赋，于是男主失去了信心并放弃了FC竞技。</p><p>在晶也第一次遇到明日香时，第一反应是明日香应该就是男主当年遇到的人，但是明日香展现出的对飞行鞋一无所知可以发现并不是如此。而在班级里，男主同级生<strong>鸢泽美咲</strong>是一位天才少女，对于学习、运动以及FC都有着出色的能力，在每一项任务中美咲都不需要出很多的力气就能取得很出色的成绩。从这就可以看出美咲应该就是当年晶也遇到的孩子，虽然在晶也的印象中是一个男孩子与他进行的比试，但99.99%（话不能说死）一定是男主记错了。</p><p>美咲有一个喜欢她的学妹<strong>有坂真白</strong>，视晶也为敌人并爱玩游戏。晶也在某一天上学时见到一位飞行姿势特别优美的少女，这名少女<strong>市之濑莉佳</strong>搬到了他家隔壁居住，她是高藤学院的FC部成员。就此，四位可攻略的女主角全部出现。</p><p>在共通线（第六章结束前）中，晶也他们与高藤学院进行了学习交流，而美咲一直想要击败的<strong>真藤一成</strong>（高藤学院FC会长），在和美咲的比拼中并没有用上全力，反而是与明日香的比赛中用尽了全力。同时在夏季大会中，真藤一成被<strong>乾沙希</strong>击败，沙希在比赛中完美操控了所有的节奏，这让一成感到非常无力。同时，晶也和美咲都对这种FC方式感到很震惊，认为自己所学习的FC方式才是真正的FC，掌控别人的比赛显得不优美。但明日香却对沙希的技巧感到由衷的赞叹。</p><p>到此我们可以看出，虽然晶也和美咲都被称作天才，但和真正有天赋的明日香比起来却显得非常渺小。明日香能够看出其中的乐趣并希望能够掌握相关的方法。这时的晶也和美咲，虽然已经是常人眼中的天才，但在真正的天才面前却如秉烛之火。为了继续进行比赛，为了能够与沙希一决高下，女主们会来邀请晶也当她们的指导老师，从此开始不同的人物线。</p><p>整个故事走完个人最喜欢的是美咲线，给我最真实感受的也是美咲线。</p><p>在美咲线中，晶也知道了当年与他比试的那名少年，那名让他放弃了FC梦想，让他放弃了在天空中自由飞翔的少年，就是美咲。两人都是别人眼中的天才，却又都在真正的天才面前黯然失色。我本以为美咲的人设是对什么都很快上手能取得优异的成绩，但没有很好的对手让她能够在一件事情上保持高度的专注。但是在《苍彼》中，美咲和晶也一样，都会对有真正杰出才能的人感到恐惧，感到害怕。</p><p>在美咲线里，晶也和美咲两个相似的人，都会对天才感到恐惧害怕的人，互相扶持着对方，成为对方的助力。晶也对美咲的FC教学中，发现了破解沙希套路的方式，而美咲这次有了晶也的鼓励和陪伴也不再畏手畏脚，希望能够完成晶也的梦想而努力练习，并最终击败了沙希和明日香取得了冠军。</p><p>对天才、对比自己更有才能的人感到恐惧、嫉妒对普通人来说是一件非常正常的事情。作为比普通人更强、在很多人严重都是天才的晶也和美咲而言，他们对明日香的感触就和普通人对他们的感触一样，只是对于他们而言很少会感受到这种情感罢了。晶也在美咲面前选择了退缩，美咲在明日香面前也选择了退缩。当人们对自己感到无法登上的高峰时，选择放弃也是很正常的，坚持可能会让你遍体鳞伤却不能达到目的，但很有可能会让你征服这座山峰。晶也后悔了，后悔自己没有继续放手一搏，他不想让自己喜爱的美咲也走上他这一条老路，毕竟放弃意味着遗憾而不是放下。美咲也为了完成晶也未实现的梦想而继续努力拼搏，面对天才们也不露怯色迎难而上。</p><p>很多人说美咲线是普通人的一条线，她胆怯，她退缩，但她遇到了晶也能够让她迎难而上。但我觉得美咲线并不是普通人的线，因为她曾是天才，她对很多事物都能够不花费力气就取得很好的成果。在他人的眼中已然是个天才的她，已然是个天才的晶也也只是比普通人更有一丝天赋罢了。我们见到晶也、美咲的时候都会自嘲无法追赶而停下努力，更何况遇到真正的天才呢？觉得与大佬们之间差距很大是不假，但巨擘们也是通过一点一滴的努力攀登到现在的位置。可能努力的结果会让我们发现自己的渺小，但至少在过程中看到了风景，也能有稍许自己的积淀。</p><p>在明日香线中，天才的光辉让晶也有了进一步执教的期望。但新的FC模式（沙希带来）的出现让晶也他们都存在一些心结，而明日香的训练和进步却让他们一起将这个难题化解。对待更新事物的乐观态度，内心的兴趣与渴望才是做成一件事的重中之重。在感情线中，明日香对晶也的表白也是剧情发展的产物（没错，真白线和莉佳线我都觉得很突兀）。</p><p>但对于明日香线，个人感触反而是：看，这就是天赋。明日香的FC天赋过于耀眼，在最终对战沙希的时候也能够较为从容的应对。全线的描述也能够看出明日香自身绽放出的光芒，但这却让我怀疑：沙希，天才的强大，只有靠真正的天才才能将其击败吗？当然若是努力的天才能够轻易的被普通人击败却也显得很不现实，但整条线给我的感触是只有真正非常具有天赋的人才能够努力去超越那一道鸿沟。相比之下，美咲线给本人的感触要更为深刻。</p><p>至于真白线和莉佳线，我只想说：真白好可爱，莉佳也可爱。</p>]]></content>
    
    
    <categories>
      
      <category>review</category>
      
    </categories>
    
    
    <tags>
      
      <tag>game</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>拖油瓶</title>
    <link href="/2022/06/25/%E6%8B%96%E6%B2%B9%E7%93%B6/"/>
    <url>/2022/06/25/%E6%8B%96%E6%B2%B9%E7%93%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="继母的拖油瓶是我的前女友"><a href="#继母的拖油瓶是我的前女友" class="headerlink" title="继母的拖油瓶是我的前女友"></a>继母的拖油瓶是我的前女友</h1><blockquote><p>甜甜的恋爱喜剧谁不喜欢呢！</p></blockquote><h1 id="故事概要"><a href="#故事概要" class="headerlink" title="故事概要"></a>故事概要</h1><p><strong>伊理户水斗</strong>和<strong>绫井结女</strong>都是阅读爱好者，两人初中时在图书馆相遇，水斗发现结女正在努力地获取书柜最上一层的一本书（好老的套路岂可修！），于是帮忙将书拿下并知道了结女是一个推理小说爱好者。两人通过读书结缘，并成为了情侣，但最终却因各种摩擦选择了分手。</p><p>伊理户水斗是一个单亲家庭，母亲在分娩后因虚弱而去世。绫井结女的父母不和选择了离婚，她跟随母亲一起生活。在中考完后，水斗的父亲告诉水斗自己准备再婚，但对方是一个离异家庭，还有一个同龄的女生会一块儿生活。水斗意识到父亲通知自己的时间点，希望父亲在之后能过的更快乐欣然接受了这个事实（毕竟更期待轻小说中的美少女搬来我家的事情发生在自己身上）。但随之而来的，却是绫井结女——不，应该是<strong>伊理户结女</strong>和她的母亲。于是前情侣在希望自己父母有幸福生活的同时过上了麻烦的同居日子。（初中生恋爱怎么会让家里知道！）</p><hr><h1 id="阅读与简评"><a href="#阅读与简评" class="headerlink" title="阅读与简评"></a>阅读与简评</h1><p><em>以下内容尽量不包含剧透（意思是肯定会有一点）</em></p><p>水斗与结女并不是因为感情断了而选择的分手，却是因为各自性格的缺陷和对自身认识而最终走上了这一条路。两人在一起生活后也更认识到了这一点，虽然对各自仍然会流露出厌烦的情绪，但这仅仅是一直生活着的两人稀松平淡的日常罢了。</p><p>结女与水斗为了不进入同一所高中而努力，却同时选择了偏差值很高的学校，获得了两个数量有限的免费生名额。结女在这一年中有了很大的变化，梳妆打扮与初中时有着天差地别，而年级第一美少女的身份也让她在刚进入高中时收货到了很大的关注。在高中开学后<strong>川波小暮</strong>和<strong>南晓月</strong>成为了伊理户兄妹（姐弟）的好友。川波与南是邻居，是青梅竹马，两人的关系也和伊理户两人类似。在两人的见证和帮忙（有时帮倒忙）下，水斗与结女更进一步认识到了自己的内心。</p><p><strong>东头伊佐奈</strong>是同级生，也是一个阅读爱好者，与结女不同的是她喜欢的是轻小说类型。在相似的场景下，水斗结识了东头并如当初见到结女一般帮助东头拿到了相关的书籍（哦豁这一幕被结女看到了胃疼胃疼胃疼）。在南与结女的鼓动下，东头向水斗表白了，水斗**********。在描写东头与水斗，水斗与结女这一段的时候，虽然通过片段的描绘就能够知道水斗和结女两人的想法，但仍然能够让我怀有期待的心情看下去，并且从此刻开始我认为水斗真正开始直面自己的内心。在这个事件之后东头经常跑去水斗家并成功让伊理户家的父母错误的知道了他们的关系。</p><p>《拖油瓶》一书中对于水斗与结女、川波与南的描写源自生活也超脱于生活。两人的情感是真实的，两人的性格是真实的，两人的所作所为也是真实的。当然现实可能不存在这么多的巧合，但“我们所度过的每一个日常，也许就是连续发生的奇迹”。让我认识到超脱于生活却是来自于东头一角色。作为与结女有着部分重叠的另一个个体，一个同样喜欢水斗的人，在之后却能够与水斗做出超脱于普通朋友的举动，结女只是感到无奈而水斗却真心觉得只是因为她是东头而已。在现实生活当中几乎不会出现这样的场景，无论是水斗还是结女基本上都不会在现实生活出现类似的表现。但《拖油瓶》仅仅是一个轻小说，一本给我们带来乐趣的小说，增加了东头这样一位性格鲜明、行动饱满的角色会让整个剧情更加生动，纸城的写作和大佬们的翻译能够让读者在整个看书的过程中并不会出现太大的和现实世界有冲突的违和感。</p><p>走进自己的内心，与他人沟通才会做出不会后悔的决定呐。</p><p>当然只要人还活着都能够尽量去弥补呢。</p><p>总之就是非常好看！（老夫的少女心）</p><p>并且全文有很多的伏笔，总体观感非常棒，不愧是恋爱喜剧部门大赏呢！期待经费正常的动画化。即使你换监督，中途缺人，我也觉得按照小说原案也能做得非常优秀了。</p>]]></content>
    
    
    <categories>
      
      <category>review</category>
      
    </categories>
    
    
    <tags>
      
      <tag>light novel</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>樱花庄的宠物女孩</title>
    <link href="/2022/06/25/%E6%A8%B1%E8%8A%B1%E5%BA%84%E7%9A%84%E5%AE%A0%E7%89%A9%E5%A5%B3%E5%AD%A9/"/>
    <url>/2022/06/25/%E6%A8%B1%E8%8A%B1%E5%BA%84%E7%9A%84%E5%AE%A0%E7%89%A9%E5%A5%B3%E5%AD%A9/</url>
    
    <content type="html"><![CDATA[<h1 id="樱花庄的宠物女孩"><a href="#樱花庄的宠物女孩" class="headerlink" title="樱花庄的宠物女孩"></a>樱花庄的宠物女孩</h1><br /><blockquote><p>天才身边总伴随着风暴，靠近者或不自知或遍体鳞伤。</p></blockquote><h1 id="故事概要"><a href="#故事概要" class="headerlink" title="故事概要"></a>故事概要</h1><p>就读于水明艺术大学附属高中的<strong>神田空太</strong>，由于夏天时在宿舍中养猫，而被校长叫去谈话，校长让他在丢掉猫与从此搬到”樱花庄”中作选择。身为爱猫一族的空太，暂时选择了流落到因聚集各种怪人而恶名昭著的“樱花庄”。 随后，天才画家<strong>椎名真白</strong>转校后住进了樱花庄，宠物女孩的到来让空太变得忙碌了起来。</p><p>樱花庄，虽是学校中被誉为“怪人聚集地”的宿舍，但在中居住的却都是在某方面有十分杰出才能的人。“怪人聚集地”或称之为“天才聚集地”更为贴切，不过天才们总会有特殊的性格，而男主确实是作为一个普通人在里面拼搏着。</p><ul><li><strong>神田空太</strong>，目标游戏制作人，普通人</li><li><strong>椎名真白</strong>，转型漫画，画家，天才</li><li><strong>青山七海</strong>，目标声优，普通人</li><li><strong>上井草美咲</strong>，动画制作人，天才</li><li><strong>三鹰仁</strong>，脚本，普通人以上，天才以下</li><li><strong>赤坂龙之介</strong>，程序员，<del>天才</del>，神</li></ul><hr><h1 id="观看与感受"><a href="#观看与感受" class="headerlink" title="观看与感受"></a>观看与感受</h1><p><em><strong>以下内容包含剧透</strong></em></p><p>看了樱花庄才明白，有才能的人就在我们身边，我们只是觉得对我们普通人来说太过遥远而无法接近罢了。而在樱花庄里，在天才们和他们身边的人们的故事里，空太这类普通人的内心活动以及天才们超越我们想象的努力却是在这部作品中很好地呈献给了读者。</p><p>给我印象最深的线并不是真白，空太或是七海，而是美咲和仁。美咲和仁互相喜欢，但他们的感情线并不能够给人很深的印象。美咲是一个天马行空的动画制作者，她做出的动画都特别有张力有表现力，并且有非常丰富的想象力。仁则是一个剧本撰写者，为美咲的动画写上能够与之匹配故事。两人制作的动画都能够有非常好的表现。美咲虽然性格大大咧咧，但一涉及到动画就会表现得非常专业；仁并不是一个剧本天才，他只是努力让写的剧本能够追上美咲的动画罢了。他努力，努力写出优秀的剧本；他懊悔，懊悔自己写的剧本总不能配上美咲的动画；他害怕，害怕因为自己的剧本而拖了整个动画的后腿。仁在他人眼中是一个天才，光芒万丈，但只有他自己觉得自己在美咲面前仅仅是个普通人，是个奋力去追赶却又无法跟上天才的普通人罢了。仁代表着有才能的普通人，他们很有能力，遍体鳞伤追赶天才的脚步会让人们觉得他们也是天才的一员，却不知他们的内心已斑驳不堪。但若他们没遇到天才，没有奋力去追赶，获取就是稍好一些的普通人碌碌一生吧。</p><p>七海和空太则是典型的普通人的代表，在七海追求声优的道路上和空太游戏制作的道路上都遇到了普通人一样的坎坷。不杰出，没有特色，缺乏亮点，总有更好的人能够替代。空太的游戏制作路上在真白给他作画后，人们看中的确实真白的作画而与其谈合作，这对人的内心会有非常大的打击。但真白却也离不开空太，天才画家转变为漫画家却也没有那么顺利。天才也许不喜欢自己的领域，转变也并不是很顺利，而空太则是一步步带领空白走向了成功。在遇到这些天才们后，空太发现了自己差距的巨大，但和七海一样他们一步一个脚印朝着自己的梦想进发。</p><p>樱花庄描述的是天才们和他们身边的人的故事，对普通人、天才的描写和故事情节都有很好的描述和掌控，对不同人的刻画也很到位，让我认识到天才们真正强大的地方以及那些一直追逐天才脚步的人们内心的强大。想起曾经和自己有过些许交集的那些有才能的人们，但我并不像空太他们一样有着能站在天才们身边的想法，想的是天才们和我有什么关系，我只是想安稳过了一生拔了，或许这才是我们这些普通人的想法吧。</p><p>当然小说、动画的世界还是过于理想画的，不过整部作品能够让我在其中忘掉现实并能乐在其中就足够了。</p><p>龙之介永远滴神！</p><p>最后心疼七海。</p>]]></content>
    
    
    <categories>
      
      <category>review</category>
      
    </categories>
    
    
    <tags>
      
      <tag>anime</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PIFu/PIFuHD</title>
    <link href="/2022/06/25/PIFu-PIFuHD/"/>
    <url>/2022/06/25/PIFu-PIFuHD/</url>
    
    <content type="html"><![CDATA[<h1 id="PIFu-x2F-PIFuHD"><a href="#PIFu-x2F-PIFuHD" class="headerlink" title="PIFu&#x2F;PIFuHD"></a>PIFu&#x2F;PIFuHD</h1><blockquote><p>PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization</p></blockquote><blockquote><p>PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization</p></blockquote><h1 id="Target"><a href="#Target" class="headerlink" title="Target"></a>Target</h1><p>从单张或多张RGB图像和图像中对应的人体mask信息，重建出图片或图片组中的人体三维模型和纹理信息。</p><blockquote><p>PIFuHD没有纹理信息，但可以使用PIFu的方式同样生成</p></blockquote><h1 id="Pixel-Aligned-Implicit-Function"><a href="#Pixel-Aligned-Implicit-Function" class="headerlink" title="Pixel-Aligned Implicit Function"></a>Pixel-Aligned Implicit Function</h1><p>Implicit Function是用于表达物体表面的函数，如$f(\cdot)$，其中$f(x)&#x3D;0$代表x在物体表面，$f(x)&lt;0$代表$x$在物体内部，$f(x)&gt;0$代表$x$在物体外部。</p><p>PIFu方法中提出的 PIFu则是讲2D图像中的Pixel也引入进来，所以叫做 Pixel-Aligned，其函数形式如下：</p><p>$f(F(x),z(X))&#x3D;s,s \in R \ \ \ \ \ \ \ (1)$</p><p>$x&#x3D;\pi(X)$为3D点X在2D图像上的投影，$z(X)$表示在此2D图片中对应的相机坐标系下的深度值。$F(x)&#x3D;g(I(x))$表示2D图片在[公式]处的深度学习的特征向量，$g(\cdot)$是由一个全卷积网络组成。</p><p>PIFu，就是对于任意一个3D点$X_i$，先根据相机参数投影得到该点的2D点位置$x_i$以及在该相机下的深度$d_i$，同时找到该2D点位置的图像特征向量$v_{x_i}$， PIFu输出$f(v_{x_i},d_i)$表示该点是否在物体表面。</p><p>论文表示PIFu关键在于输入的Pixel-Aligned图像特征向量$v_{x_i}$，学习得到的$f(\cdot)$可以在重建的模型中很好地保留图片当中呈现的一些细节。同时PIFu这种连续性地本质可以用一种使用内存较少的方式重建任意拓扑结构的几何信息，并且公式（1）中$s$可以替换为如rgb等数据。</p><h1 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h1><p><img src="/./2022/06/25/PIFu-PIFuHD/PIFu.png" alt="PIFu"></p><h2 id="单视角重建"><a href="#单视角重建" class="headerlink" title="单视角重建"></a>单视角重建</h2><p>PIFu文章使用的表面GT Implicit Function:</p><p>$$<br>f_v^*(x)&#x3D;\left{\begin{aligned}1&amp;,\ if\ X\ is\ inside\ mesh\ surface\0&amp;,\ otherwise\end{aligned}\right.<br>$$</p><p>$f_v^*&gt;0.5$表示该点在物体内部；$f_v^*&lt;0.5$表示该点在物体外部。这样的能将Implicit Function的值转变成了该3D点被物体占用的概率，值越靠近1表示被占有的概率越大，值越靠近0，则是空白区域的几率也越大，方便网络最后Sigmoid输出。</p><p>有的训练数据：m个对应的pair，(2D图像，3D模型)。</p><p>根据上图中PIFu 表面重建的流程： 对于每一个(2D图像，3D模型)，2D图像会先输入到一个由全卷积组成的image encoder $g$，得到与原2D图大小一致的深度特征$F_V\in R^{h\times w\times c}$。对于3D模型，可以采样得到n个3D点${X_1,X_2,…,X_n}$， 同时可以知道其对应的Implicit Function的GT${f_v^*(X_1),f_v^*(X_2),…,f_v^*(X_n)}$</p><p>于是论文构建损失函数:</p><p>$$<br>L_v&#x3D;\frac{1}{n}\sum_{i&#x3D;1}^{n}{|f_v(F_V(x_i),z(X_i))-f_v^*(X_i)|}^2<br>$$</p><p>论文使用MLP去拟合上述的方程$f_v(\cdot)$。在梯度下降过程中，image encoder $g$和$f_v(\cdot)$是同时联合优化的（Surface Reconstruction）。image encoder $g$和$f_v(\cdot)$训练好之后，在推理阶段，输入就是一张图片和对应的相机参数，以及该图片中人体所处的一个大致范围（Bounding Box，$[x_{min},x_{max},y_{min},y_{max},z_{min},z_{max}]$）。对于Bounding Box，我们可以在三个维度上进行离散化，比如每个维度512，就可以得到$512\times512\times512$个点$X_i$，对所有的点输入到$f_v(\cdot)$函数中，那就可以得到3D Occupacy信息，然后跑一遍Marching Cube就可以得到物体的模型。</p><h2 id="颜色重建"><a href="#颜色重建" class="headerlink" title="颜色重建"></a>颜色重建</h2><p>将公式（1）中的$s$改成$rgb$，来获取3D点对应的颜色预测。</p><p>损失函数为:</p><p>$$<br>L_c&#x3D;\frac{1}{n}\sum_{i&#x3D;1}^{n}{|f_c(F_C(x_i),z(X_i))-C(X_i)|}<br>$$</p><p>其中$C(X_i)$表示的是3D点$X_i$在2D图像中投影的颜色。同样的也可以采用和表面几何重建的方式，同时优化image encoder 和 implicit function $f_c(\cdot)$（Texture Inference）。文章中提出，是用这种方式这样会很容易过拟合，这是因为在这种情况下$f_c(\cdot)$既要预测表面顶点的颜色，又要去学习物体潜在的3D信息，这样它才可以推断不同pose和不同形状的人体的不可见部分（比如背面顶点）的颜色信息。于是文章提出了另外一种方式，就是基于上一步骤中的到用于重建的深度特征$F_V$。在这种情况下$f_c(\cdot)$只需要关注顶点的颜色，而不需要去关注潜在的3D信息，所以上面的损失函数变成：</p><p>$$<br>L_c&#x3D;\frac{1}{n}\sum_{i&#x3D;1}^{n}{|f_c(F_C(x_i^{‘},F_V),z(X_{i,z}^{‘}))-C(X_i)|}<br>$$</p><p>表面顶点的$X_i^{‘}&#x3D;X_i+\epsilon \cdot N_i$，其中$\epsilon \sim N(0,d)$，添加了一个偏移量使得该点的周围都是该颜色，能够让网络更好的学习出相关的特征。</p><h2 id="多视角重建"><a href="#多视角重建" class="headerlink" title="多视角重建"></a>多视角重建</h2><p><img src="/./2022/06/25/PIFu-PIFuHD/pixelalign.png" alt="multiview"></p><p>多视角下，PIFu变成$f(mean(\Phi_{view_1},\Phi_{view_2},…,\Phi_{view_n}))&#x3D;s$，即每一个点的特征为所有视角下该点特征的平均。</p><h2 id="PIFuHD"><a href="#PIFuHD" class="headerlink" title="PIFuHD"></a>PIFuHD</h2><p>由于显卡内存的限制，PIFu的输入图片的大小为$512\times512$，输出的特征为$128\times128$，且使用的是HourGlass作为image encoder，该网络结果可以达到整张图片的感受野，并且多对中间阶段输出的监督可以得到比较鲁棒的3D重建结果，但是这样的网络结构也限制了该方法输入更大分辨率的输入图片，同时也不能得到更高维度的特征。</p><p><img src="/./2022/06/25/PIFu-PIFuHD/PIFuHD.png" alt="PIFuHD"></p><p>多层PIFu，输入是$1024\times1024$的图片$I_H$，输出是高精度的人体三维模型。</p><ul><li><p>Coarse Level</p><p>  与PIFu相同，输入的图片大小为$512\times512$，为原图经过下采样后得到的图片（$I_L$），输出特征为$128\times128$。途中输入部分还添加了$512\times512$图片预测得到的正面、反面法相图$F_L,B_L$，此模块为:</p><p>  $$<br>  f^L(X)&#x3D;g^L(\Phi^L(x_L,I_L,F_L,B_L,),Z)<br>  $$</p><p>  其中$x_L\in R^2$是3D点$X$在$I_L$图片中2D的投影位置，$Z$为该相机视角下的深度，$\Phi^L$是提取图像特征的神经网络，$g^L$是MLP，从$X$的特征和投影深度$Z$判断该3D点是否在需要建模的人体内部</p></li><li><p>Fine Level</p><p>  这个部分是的主干网络输入是原始的$1024\times1024$的图片$I_H$，输出$512\times512$大小的图像特征，得到大分辨率的特征图是为了能够得到三维模型上的精细细节。同样这个模块也输入了高分辨率的正面、反面法向图$F_H,B_H$:</p><p>  $$<br>  f^H(X)&#x3D;g^H(\Phi^H(x_H,I_H,F_H,B_H,),\Omega(X))<br>  $$</p><p>  其中$x_H\in R^2$是3D点$X$在$I_H$图片中2D的投影位置，且$x_H&#x3D;2x_L$。</p><p>  $\Phi^H$和$\Phi^L$有着相似的网络结构，主要的不同在于$\Phi^H$感受野没有覆盖这个图片，但是由于卷积网络的特性，可以通过滑动窗口的方式提取整张图像的特征。通过这种方式得到的特征就没有全局的信息，所以在$g^H$中，没有直接输入投影的深度$Z$，而是将coarse level中的全局特征$\Omega(X)$引入，以获取全局的特征信息。</p></li></ul><p>由于Fine Level输入了Coarse Level的3D全局特征，所以经过Fine Level重建出来的结果不会差于Coarse Level单独的输出结果。如果网络设计能够很好利用高分辨率的图像信息，可以生成更好的三维模型。 另外，Fine Level不需要归一化可以让这一部分网络使用图片crop去进行训练，节省了显存。</p><ul><li><p>Front-to-Back推理</p><p>  在Coarse Level和Fine Level都是用了图片的正面、反面法相图，论文使用pix2pixHD网络将RGB图像预测出图片的正面和反面法相图。</p></li></ul><p>在PIFuHD中使用了扩展BCE损失函数，即</p><p>![extbce][.&#x2F;2022&#x2F;06&#x2F;25&#x2F;PIFu-PIFuHD&#x2F;extbce.png]</p><p>在in&#x2F;out采样数量不同的时候，会给数量较少的一方有更大的权重（类似focal loss）。</p><ul><li>pix2pix</li></ul><p>pix2pix理解成image translate，在PIGFuHD中，利用一个Generator</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://zhuanlan.zhihu.com/p/148509062">论文解读 | PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization</a></p><p><a href="https://zhuanlan.zhihu.com/p/149657262">论文解读 | PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization</a></p><p><a href="https://arxiv.org/pdf/1611.07004.pdf"></a></p>]]></content>
    
    
    <categories>
      
      <category>paper reading</category>
      
    </categories>
    
    
    <tags>
      
      <tag>volume reconstruction</tag>
      
      <tag>paper reading</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TSDF</title>
    <link href="/2022/06/25/TSDF/"/>
    <url>/2022/06/25/TSDF/</url>
    
    <content type="html"><![CDATA[<h1 id="TSDF"><a href="#TSDF" class="headerlink" title="TSDF"></a>TSDF</h1><p>TSDF ，即Truncated Signed Distance Function，基于截断的带符号距离函数，是一种常见的在3D重建中计算隐势面的方法。TSDF是在SDF进行改进的，在拥有大内存的显卡并行计算的情况下，使用TSDF可以做到实时的重建效果，获得了很多方面的落地使用。</p><h1 id="算法思想"><a href="#算法思想" class="headerlink" title="算法思想"></a>算法思想</h1><p>TSDF的算法的思路就是用一个大的空间（Volume）作为要建立的三维模型，这个空间可以完全包括我们的模型，Volume由许多个小的体素（Voxel）组成。</p><p>每个Voxel对应空间中一个点，这个点我们用两个量来评价：</p><ol><li>该Voxel到最近的Surface（一般称作Zero Crossing）的距离，记作TSDF(x)，即带符号距离</li><li>Voxel更新时的权重，记作w</li></ol><p><img src="/./2022/06/25/TSDF/distance.png" alt="distance"></p><p>假设我们真实的面到相机的深度是ds，相机采集到的深度dv，那么符号距离值就是：</p><p>$d(x)&#x3D;ds-dv$</p><p>当d(x)&gt;0 时说明该体素在真实的面的前面，小于0 ，则说明该体素在真实的面的后面</p><p>每一次相机采集出来的数值，我们都认为是最大可能真实面，在相机前后也有可能是真实面，但是概率要小。这个前后距离我们对它进行一定的限制，因为离得特别远的话，其概率也是很小，我们就忽略了。</p><p><img src="/./2022/06/25/TSDF/TSDF.png" alt="TSDF"></p><p>在计算新的拍摄帧的体素的符号距离值和体素更新的过程中，我们不是所有的体素都查找和更新，而是只查找更新截断距离内的体素，这也是TSDF与SDF不同的一点，大大缩短了计算量，并提高了精度</p><h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h1><ol><li><p>准备工作</p><ul><li>建立长方体包围盒，能够完全包围要重建的物体</li><li>划分网格体素，对包围盒划分n等分，体素的大小取决于包围盒和划分体素的数目决定。我们将整个空间的体素全部存入GPU运算，每个线程处理一条(x,y)。即对于(x,y,z)的晶格坐标，每个GPU进程扫描处理一个(x,y)坐标下的晶格柱</li><li>对于构造的立体中的每个体素g，转化g为世界坐标系下得三维位置点 p（根据体素的大小，以及体素的数目）</li></ul></li><li><p>计算当前帧的TSDF值以及权重</p><p> 这一步我们遍历所有的体素，以一个体素在世界坐标系三维位置点p为例</p><ul><li>由深度数据的相机位姿矩阵，求世界坐标系下点p在相机坐标系下得映射点v，并由相机内参矩阵，反投影v点求深度图像中的对应像素点x，像素点x的深度值为value(x)，点v到相机坐标原点的距离为distance(v)</li><li>位置点$p$的SDF值为$SDF(p)&#x3D;value(x)-distance(v)$。现在我们就要引入截断距离了，计算出$TSDF(p)$, 公式写出来比较复杂，直接描述就是在截断距离$u$以内，$TSDF(p)&#x3D;\frac{SDF(p)}{|u|}$， 否则，如果$SDF(p) &gt;0$，$TSDF(p)  &#x3D; 1$；$SDF(p) &lt;0$，$TSDF(p)  &#x3D; -1$</li><li>权重$w(p)$的计算公式：$w(p) &#x3D; \frac{cos(\theta)}{distance(v)}$，其中θ为投影光线与表面法向量的夹角</li></ul><p> 经过我们这一步就算出这一帧的所有体素的TSDF值以及权重值 </p></li><li><p>当前帧与全局融合结果进行融合</p><p> 如果当前帧是第一帧，则第一帧即是融合结果，否则需要当前帧与之前的融合结果在进行融合。假设TSDF‘(p)为体素p的融合TSDF值，w’(p)为融合权重值，TSDF(p)为体素p当前帧的TSDF值，w(p)为当前帧权重值。现在我们要通过TSDF(p)更新TSDF’(p)。公式如下</p><p> $$<br> \begin{aligned}TSDF’(p)&amp;&#x3D;\frac{w’(p)\cdot TSDF’(P)+w(p)\cdot TSDF(p)}{w’(p)+w(p)}\w’(p)&amp;&#x3D;w’(p)+w(p)\end{aligned}<br> $$</p><p> 通过上述公式就可以将新的帧融合进融合帧内。</p><p> 第一部分完成后，就是每添加一帧深度数据，执行一遍2、3步的计算，知道最后输出结果给Marching Cube计算提出三角面</p></li></ol><h1 id="算法特点"><a href="#算法特点" class="headerlink" title="算法特点"></a>算法特点</h1><ul><li>计算简单</li><li>需要大量并行计算加快速度</li><li>生成的网格的细节保持比较好，而且精确度也比较好，但是在边缘处以及前后景交界处，会出现较大的拖尾现象</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.jianshu.com/p/462fe75753f7">网格生成之TSDF算法学习笔记</a></p><p><a href="https://zhuanlan.zhihu.com/p/42112101">三维重建中的表面模型构建–TSDF算法</a></p>]]></content>
    
    
    <categories>
      
      <category>3D Basic</category>
      
    </categories>
    
    
    <tags>
      
      <tag>volume</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>点云配准与ICP</title>
    <link href="/2022/06/25/%E7%82%B9%E4%BA%91%E9%85%8D%E5%87%86%E4%B8%8EICP/"/>
    <url>/2022/06/25/%E7%82%B9%E4%BA%91%E9%85%8D%E5%87%86%E4%B8%8EICP/</url>
    
    <content type="html"><![CDATA[<h1 id="点云配准与ICP"><a href="#点云配准与ICP" class="headerlink" title="点云配准与ICP"></a>点云配准与ICP</h1><h1 id="关于点云"><a href="#关于点云" class="headerlink" title="关于点云"></a>关于点云</h1><ul><li>点云是在同一空间参考系下表达目标空间分布和目标表面特性的海量点集合，在获取物体表面每个采样点的空间坐标后，得到的是点的集合，称之为点云（Point Cloud）</li><li>点云一般包括根据激光测量得到的点云和根绝摄影测量得到的点云<ul><li>激光测量的点云包括三维坐标（XYZ）和激光反射强度（Intensity）。强度信息与目标的表面材质、粗糙度、入射角方向，以及仪器的发射能量，激光波长有关</li><li>摄影测量原理得到的点云，包括三维坐标（XYZ）和颜色信息（RGB）</li><li>也有把激光和摄影相结合在一起的（多传感器融合技术），这种结合激光测量和摄影测量原理得到点云，包括三维坐标（XYZ）、激光反射强度（Intensity）和颜色信息（RGB）</li></ul></li></ul><h1 id="什么是点云配准"><a href="#什么是点云配准" class="headerlink" title="什么是点云配准"></a>什么是点云配准</h1><p>点云配准（Point Cloud Registration）指的是输入两幅点云$P_s$（Source）和$P_t$（Target） ，输出一个变换$T$使得$T(P_s)$和$P_t$的重合程度尽可能高。变换$T$可以是刚性的（Rigid），也可以不是，下面只考虑刚性变换，即变换只包括旋转（$R$）、平移（$t$）。</p><p>点云配准可以分为粗配准（Coarse Registration）和精配准（Fine Registration）两步。粗配准指的是在两幅点云之间的变换完全未知的情况下进行较为粗糙的配准，目的主要是为精配准提供较好的变换初值；精配准则是给定一个初始变换，进一步优化得到更精确的变换。</p><p>目前应用最广泛的点云精配准算法是迭代最近点算法（Iterative Closest Point，ICP）及各种变种ICP算法。</p><h1 id="ICP，局部点云配准"><a href="#ICP，局部点云配准" class="headerlink" title="ICP，局部点云配准"></a>ICP，局部点云配准</h1><p>ICP，Iterative Closest Point，即迭代最近点算法，是应用最广泛的3D点云配准算法之一， 其通过欧式变换求解出两片点云的旋转平移矩阵及对应的配准误差。</p><h2 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h2><p>对于$T$是刚性变换的情形，点云配准问题可以描述为</p><p>$$<br>R^*,t^*&#x3D;\mathop{arg\ min}\limits_{R,t}\frac{1}{|P_s|}\sum_{i&#x3D;1}^{|P_s|}||p_t^i-(R\cdotp_s^i+t)||^2<br>$$</p><p>其中$p_s,p_t$是源点云和目标点云中的对应点</p><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><ol><li>点云预处理，如滤波、清洗等</li><li>利用一些点解出的变换，寻找最近点</li><li>调整部分对应点的权重</li><li>删除不合理对应点</li><li>计算Loss，最小化Loss并求解当前最优变换</li><li>重复2-6，迭代收敛</li></ol><h2 id="寻找最近点"><a href="#寻找最近点" class="headerlink" title="寻找最近点"></a>寻找最近点</h2><p>利用初始$R_0,t_0$或上一次迭代得到的$R_{k-1},t_{k-1}$对初始点云进行变换，得到一个临时的变换点云，然后用这个点云和目标点云进行比较，找出源点云中每一个点在目标点云中的最近邻点。</p><p>如果直接进行比较找最近邻点，需要进行两重循环，计算复杂度为 $O(|P_s|\cdot |P_t|)$，这一步会比较耗时，常见的加速方法有</p><ul><li>设置距离阈值，当点与点距离小于一定阈值就认为找到了对应点，不用遍历完整个点集</li><li>使用 ANN(Approximate Nearest Neighbor) 加速查找，常用的有 KD-tree；KD-tree 建树的计算复杂度为$O(N\log(N))$，查找通常复杂度为$O(\log(N))$，最坏情况下$O(N)$</li></ul><h2 id="求解最优变换"><a href="#求解最优变换" class="headerlink" title="求解最优变换"></a>求解最优变换</h2><p>对于Point-to-Point ICP问题，求最优变换是有闭形式解（Closed-Form Solution）的，可以借助SVD分解来计算。</p><ul><li><p>结论</p><p>  在已知点的对应关系的情况下，设$\overline{p}_s,\overline{p}_t$分别表示源点云和目标点云的质心，令$\hat{p}_s^i&#x3D;p_s^i-\overline{p}_s^i\ ,\ \ \hat{p}_t^i&#x3D;p_t^i-\overline{p}<em>t^i\ ,\ \ H&#x3D;\sum</em>{i&#x3D;1}^{|P_s|}\hat{p}_s^i{\hat{p}_t^i}^T$，这是一个$3\times 3$矩阵，对$H$进行SVD分解得到$H&#x3D;U\Sigma V^T$，则Point-to-Point ICP的最优解为</p><p>  $$<br>  R^*&#x3D;VU^T<br>  $$</p><p>  最优平移为</p><p>  $$<br>  t^*&#x3D;\overline{p}_t-R^*\cdot\overline{p}_s<br>  $$</p></li><li><p>最优平移</p><p>  令$N&#x3D;|P_s|$，设$F(t)&#x3D;\sum_{i&#x3D;1}^N||(R\cdot p_s^i+t)-p_t^i||^2$，对其进行求导，则有</p><p>  $$<br>  \begin{aligned}\frac{\partial F}{\partial t}&amp;&#x3D;\sum_{i&#x3D;1}^N2(R\cdotp_s^i+t-p_t^i)\&amp;&#x3D;2nt+2R\sum_{i&#x3D;1}^Np_s^i-2\sum_{i&#x3D;1}^Np_t^i\end{aligned}<br>  $$</p><p>  令导数为0，得</p><p>  $$<br>  \begin{aligned}t&amp;&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^Np_t^i-R\frac{1}{N}\sum_{i&#x3D;1}^Np_s^i\&amp;&#x3D;\overline{p}_t^i-R\cdot\overline{p}_s^i\end{aligned}<br>  $$</p><p>  无论$R$取值如何，根据上式我们都可以求得最优的 t，使得 loss 最小</p></li><li><p>最优旋转</p><p>  经过最优平移的推导，我们知道无论旋转如何取值，都可以通过计算点云的质心来得到最优平移，为了下面计算上的简便，我们不妨不考虑平移的影响，先将源点云和目标点云都转换到质心坐标下，这也就是之前令$\hat{p}_s^i&#x3D;p_s^i-\overline{p}_s^i\ ,\ \ \hat{p}_t^i&#x3D;p_t^i-\overline{p}_t^i$的意义。</p><p>  不考虑平移，则Loss可以写成</p><p>  $$<br>  F(R)&#x3D;\sum_{i&#x3D;1}^N||R\cdot \hat{p}_s^i-\hat{p}_t^i||^2<br>  $$</p><p>  化简有</p><p>  $$<br>  \begin{aligned}||R\cdot \hat{p}_s^i-\hat{p}_t^i||^2&amp;&#x3D;(R\cdot \hat{p}_s^i-\hat{p}_t^i)^T(R\cdot \hat{p}_s^i-\hat{p}_t^i)\&amp;&#x3D;({\hat{p}_s^i}^TR^T-{\hat{p}_t^i}^T)(R\cdot \hat{p}_s^i-\hat{p}_t^i)\&amp;&#x3D;{\hat{p}_s^i}^TR^TR\hat{p}_s^i-{\hat{p}_t^i}^TR{\hat{p}_s^i}-{\hat{p}_s^i}^TR^T{\hat{p}_t^i}+{\hat{p}_t^i}^T{\hat{p}_t^i}\&amp;&#x3D;||{\hat{p}_s^i}||^2+||{\hat{p}_t^i}||^2-{\hat{p}_t^i}^TR{\hat{p}_s^i}-{\hat{p}_s^i}^TR^T{\hat{p}_t^i}\&amp;&#x3D;||{\hat{p}_s^i}||^2+||{\hat{p}_t^i}||^2-2{\hat{p}_t^i}^TR{\hat{p}_s^i}\end{aligned}<br>  $$</p><p>  其中，$R^TR&#x3D;I,{\hat{p}_t^i}^TR{\hat{p}_s^i}&#x3D;{\hat{p}_s^i}^TR^T{\hat{p}_t^i}$</p><p>  由于点的坐标是确定的（和$R$无关），所以最小化原Loss等价于求</p><p>  $$<br>  R^*&#x3D;\mathop{arg \ min}\limits_{R}(-2\sum_{i&#x3D;1}^N{\hat{p}_t^i}^TR{\hat{p}_s^i})<br>  $$</p><p>  由于$trace(AB)&#x3D;trace(BA)$，即有</p><p>  $$<br>  \begin{aligned}R^*&amp;&#x3D;\mathop{arg \ max}\limits_{R}(\sum_{i&#x3D;1}^N{\hat{p}<em>t^i}^TR{\hat{p}<em>s^i})\&amp;&#x3D;\mathop{arg \ max}\limits</em>{R} \ trace(P_t^TRP_s)\&amp;&#x3D;\mathop{arg \ max}\limits</em>{R} \ trace(RP_sP_t^T)\end{aligned}<br>  $$</p><p>  带入SVD分解，得</p><p>  $$<br>  \begin{aligned}trace(RP_sP_t^T)&amp;&#x3D;trace(RH)\&amp;&#x3D;trace(RU\Sigma V^T)\&amp;&#x3D;trace(\Sigma V^TRU)\end{aligned}<br>  $$</p><p>  由于$V,U,R$都是正交矩阵，所以$V^TRU$也是正交矩阵，令</p><p>  $$<br>  M&#x3D;V^TRU&#x3D;\left[\begin{aligned}m_{11}&amp; &amp;m_{12}&amp; &amp;m_{13}\m_{21}&amp; &amp;m_{22}&amp; &amp;m_{23}\m_{31}&amp; &amp;m_{32}&amp; &amp;m_{33}\end{aligned}\right]<br>  $$</p><p>  则有</p><p>  $$<br>  \begin{aligned}trace(\Sigma V^TRU)&amp;&#x3D;trace(\Sigma M)\&amp;&#x3D;\sigma_1m_{11}+\sigma_2m_{22}+\sigma_1m_{33}\end{aligned}<br>  $$</p><p>  根据奇异值非负的性质和正交矩阵的性质（正交矩阵中的元素绝对值不大于 1），容易证得只有当$M$为单位阵时$trace(\Sigma M)$最大，即</p><p>  $$<br>  V^TRU&#x3D;I\R&#x3D;VU^T<br>  $$</p><p>  所以有$R^*&#x3D;VU^T$</p><p>  最后还需要进行 Orientation rectification，即验证$R^*&#x3D;VU^T$是不是一个旋转矩阵（检查是否有$|R|&#x3D;1$），因为存在$|R|&#x3D;-1$的可能，此时$R$表示的不是旋转而是一个Reflection，所以我们还要给上述优化求解加上一个$|R|&#x3D;1$的约束，详见<a href="https://www.notion.so/ICP-6b09499500b747c98ce5d0ed770e7f6b">Reference</a></p><p>  计算步骤为</p><ol><li>计算源点云和目标点云质心</li><li>将源点云和目标点云转换到质心坐标系</li><li>计算矩阵$H$</li><li>对$H$求 SVD 分解，根据公式求得$R^*$</li><li>根据公式计算$t^*$</li></ol></li><li><p>迭代</p><p>  每一次迭代我们都会得到当前的最优变换参数$R_k,t_k$，然后将该变换作用于当前源点云。找最近对应点和求解最优变换这两步不停迭代进行，直到满足迭代终止条件，常用的终止条件有</p><ul><li>$R_k,t_k$的变化量小于一定值</li><li>Loss变化量小于一定值</li><li>达到最大迭代次数</li></ul></li></ul><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>优点</p><ul><li>简单，不必对点云进行分割和特征提取</li><li>初值较好情况下，精度和收敛性都不错</li></ul><p>缺点</p><ul><li>找最近对应点的计算开销较大</li><li>只考虑了点与点距离，缺少对点云结构信息的利用</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://zhuanlan.zhihu.com/p/91275450">点云配准综述</a></p><p><a href="https://yilingui.xyz/2019/11/20/191120_point_cloud_registration_icp/">三维点云配准 – ICP 算法</a></p><p><a href="https://www.math.pku.edu.cn/teachers/yaoy/Fall2011/arun.pdf"></a></p>]]></content>
    
    
    <categories>
      
      <category>3D Basic</category>
      
    </categories>
    
    
    <tags>
      
      <tag>point cloud</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>相机标定</title>
    <link href="/2022/06/24/%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A/"/>
    <url>/2022/06/24/%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A/</url>
    
    <content type="html"><![CDATA[<h1 id="相机标定"><a href="#相机标定" class="headerlink" title="相机标定"></a>相机标定</h1><h1 id="相机标定是什么"><a href="#相机标定是什么" class="headerlink" title="相机标定是什么"></a>相机标定是什么</h1><p>相机的操作是将我们所处的三维空间中的某个场景通过某个模型转换成二维图像，并且这个3D→2D的过程是不可逆的。而相机标定的目标是寻找一个合适的数学模型，求出这个模型的参数，这样我们能够近似这个三维到二维的过程，使这个三维到二维的过程的函数找到反函数。整个逼近的过程就是相机标定，用简单的数学模型来表达复杂的成像过程，并且求出成像的反过程。标定之后的相机，可以进行三维场景的重建，即深度的感知。</p><p>在图像测量过程以及机器视觉应用中，为确定空间物体表面某点的三维几何位置与其在图像中对应点之间的相互关系，必须建立相机成像的几何模型，这些几何模型参数就是相机参数。在大多数条件下这些参数必须通过实验与计算才能得到，这个求解参数的过程就是相机标定。无论是在图像测量或者机器视觉应用中，相机参数的标定都是非常关键的环节，其标定结果的精度及算法的稳定性直接影响相机工作产生结果的准确性。</p><h1 id="坐标系"><a href="#坐标系" class="headerlink" title="坐标系"></a>坐标系</h1><ul><li><p>世界坐标系</p><p>  世界坐标系（World Coordinate）$(x_w,y_w,z_w)$，也称为测量坐标系，三维直角坐标系，以其为基准可以描述相机和待测物体的空间位置。世界坐标系的位置可以根据实际情况自由确定</p></li><li><p>相机坐标系</p><p>  相机坐标系（Camera Coordinate）$(x_c,y_c,z_c)$，三维直角坐标系，原点位于镜头光心处，$x,y$轴分别与相面的两边平行，$z$轴为镜头光轴，与像平面垂直</p></li><li><p>像素坐标系</p><p>  像素坐标系$uov$是一个二维直角坐标系，反映了相机CCD&#x2F;CMOS芯片中像素的排列情况。原点$o$位于图像的左上角，$u,v$轴分别于像面的两边平行。像素坐标系中坐标轴的单位是像素（整数）</p></li><li><p>图像坐标系</p><p>  像素坐标系不利于坐标变换，因此需要建立图像坐标系$XOY$，其坐标轴的单位通常为毫米，原点是相机光轴与相面的交点（称为主点），即图像的中心点，$X,Y$轴分别与$u,v$轴平行。故两个坐标系实际是平移关系，即可以通过平移就可得到</p></li><li><p>世界坐标系转换为相机坐标系</p><p>  $$<br>  \left[\begin{aligned}&amp;x_c\&amp;y_c\&amp;z_c\&amp;1\end{aligned}\right]&#x3D;\left[\begin{aligned}&amp;R &amp;t\&amp;0 &amp;1\end{aligned}\right]\left[\begin{aligned}&amp;x_w\&amp;y_w\&amp;z_w\&amp;1\end{aligned}\right]<br>  $$</p><p>  其中$R$为$3\times3$的旋转矩阵，$t$为$3\times1$的平移矢量，$[x_c,y_c,z_c,1]^T$为相机坐标系的齐次坐标，$[x_w,y_w,z_w,1]^T$为世界坐标系的齐次坐标</p></li><li><p>图像坐标系转换为像素坐标系</p><p>  $$<br>  \left[\begin{aligned}&amp;u\&amp;v\&amp;1\end{aligned}\right]&#x3D;\left[\begin{aligned}&amp;\frac{1}{dX} &amp;0&amp; \ &amp;u_0\&amp;0 &amp;\frac{1}{dY}&amp; \ &amp;v_0\&amp;0 &amp;0&amp; \ &amp;1\end{aligned}\right]\left[\begin{aligned}&amp;X\&amp;Y\&amp;1\end{aligned}\right]<br>  $$</p><p>  其中，$dX,dY$分别为像素在$X,Y$轴方向上的物理尺寸，$u_0,v_0$为主点（图像原点）坐标</p></li><li><p>针孔相机模型</p><p>  相机成像模型简化为针孔模型，如下图所示。</p><p>  <img src="/./2022/06/24/%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A/pinhole.png" alt="pinhole"></p><p>  空间任意一点$P$与其图像点$p$之间的关系，$P$与相机光心$o$的连线为$oP$，$oP$与像面的交点$p$即为空间点$P$在图像平面上的投影。该过程为透视投影，如下矩阵表示</p><p>  $$<br>  s\left[\begin{aligned}&amp;X\&amp;Y\&amp;1\end{aligned}\right]&#x3D;\left[\begin{aligned}&amp;f &amp;0&amp; &amp;0&amp; &amp;0\&amp;0 &amp;f&amp; &amp;0&amp; &amp;0 \&amp;0 &amp;0&amp; &amp;1&amp; &amp;0\end{aligned}\right]\left[\begin{aligned}&amp;x\&amp;y\&amp;z\&amp;1\end{aligned}\right]<br>  $$</p><p>  其中$s$为缩放因子，$f$为有效焦距（光心到图像平面的距离），$[x,y,z,1]^T$是空间点$P$在相机坐标系$oxyz$中的齐次，$[X,Y,1]^T$是图像点$p$在图像坐标系$OXY$中的齐次坐标</p></li><li><p>世界坐标系转换为像素坐标系</p><p>  $$<br>  s\left[\begin{aligned}&amp;u\&amp;v\&amp;1\end{aligned}\right]&#x3D;\left[\begin{aligned}&amp;\frac{1}{dX} &amp;0&amp; \ &amp;u_0\&amp;0 &amp;\frac{1}{dY}&amp; \ &amp;v_0\&amp;0 &amp;0&amp; \ &amp;1\end{aligned}\right]\left[\begin{aligned}&amp;f &amp;0&amp; &amp;0&amp; &amp;0\&amp;0 &amp;f&amp; &amp;0&amp; &amp;0 \&amp;0 &amp;0&amp; &amp;1&amp; &amp;0\end{aligned}\right]\left[\begin{aligned}&amp;R &amp;t\&amp;0 &amp;1\end{aligned}\right]\left[\begin{aligned}&amp;x_w\&amp;y_w\&amp;z_w\&amp;1\end{aligned}\right]\&#x3D;\left[\begin{aligned}&amp;\alpha_x &amp;0&amp; &amp;u_0&amp; &amp;0\&amp;0 &amp;\alpha_y&amp; &amp;v_0&amp; &amp;0 \&amp;0 &amp;0&amp; &amp;1&amp; &amp;0\end{aligned}\right]\left[\begin{aligned}&amp;R &amp;t\&amp;0 &amp;1\end{aligned}\right]\left[\begin{aligned}&amp;x_w\&amp;y_w\&amp;z_w\&amp;1\end{aligned}\right]&#x3D;M_1M_2X_w&#x3D;MX_w<br>  $$</p><p>  其中，$\alpha_x&#x3D;f&#x2F;dX,\alpha_y&#x3D;f&#x2F;dY$，称为$u,v$轴的尺度因子，$M_1$称为相机的内部参数矩阵，$M_2$称为相机的外部参数矩阵，$M$称为投影矩阵</p></li></ul><h1 id="畸变参数"><a href="#畸变参数" class="headerlink" title="畸变参数"></a>畸变参数</h1><p>畸变（distortion）是对直线投影（rectilinear projection）的一种偏移。简单来说直线投影是场景内的一条直线投影到图片上也保持为一条直线。畸变简单来说就是一条直线投影到图片上不能保持为一条直线了，这是一种光学畸变（optical aberration）。</p><p>畸变一般可以分为径向畸变和切向畸变。径向畸变来自于透镜形状，切向畸变来自于整个摄像机的组装过程。畸变还有其他类型的畸变，但是没有径向畸变、切向畸变显著。</p><p>实际摄像机的透镜总是在成像仪的边缘产生显著的畸变，这种现象来源于“筒形”或“鱼眼”的影响。光线在原理透镜中心的地方比靠近中心的地方更加弯曲。对于常用的普通透镜来说，这种现象更加严重。对于径向畸变，成像仪中心（光学中心）的畸变为0，随着向边缘移动，畸变越来越严重。</p><p><img src="/./2022/06/24/%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A/distortion1.png" alt="distortion1"></p><p>切向畸变是由于透镜制造上的缺陷使得透镜本身与图像平面不平行而产生的。</p><p><img src="/./2022/06/24/%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A/distortion2.png" alt="distortion2"></p><h1 id="张正友标定法"><a href="#张正友标定法" class="headerlink" title="张正友标定法"></a>张正友标定法</h1><p>张正友标定法利用如下图所示的棋盘格标定板，在得到一张标定板的图像之后，可以利用相应的图像检测算法得到每一个角点的像素坐标$(u,v)$。</p><p><img src="/./2022/06/24/%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A/zhang.png" alt="zhang"></p><p>张正友标定法将世界坐标系固定于棋盘格上，则棋盘格上任一点的物理坐标$W&#x3D;0$，由于标定板的世界坐标系是人为事先定义好的，标定板上每一个格子的大小是已知的，我们可以计算得到每一个角点在世界坐标系下的物理坐标$(U,V,W&#x3D;0)$。</p><p>我们将利用这些信息：每一个角点的像素坐标$(u,v)$ 、每一个角点在世界坐标系下的物理坐标$(U,V,W&#x3D;0)$，来进行相机的标定，获得相机的内外参矩阵、畸变参数。</p><h2 id="标定相机的内外参数的思路"><a href="#标定相机的内外参数的思路" class="headerlink" title="标定相机的内外参数的思路"></a>标定相机的内外参数的思路</h2><ol><li>求解内参矩阵与外参矩阵的积</li><li>求解内参矩阵</li><li>求解外参矩阵</li></ol><ul><li><p>求解内参矩阵与外参矩阵的积</p><p>  将世界坐标系固定于棋盘格上，则棋盘格上任一点的物理坐标$W&#x3D;0$，因此，原单点无畸变的成像模型可以化为下式。其中， $R_1,R_2$为旋转矩阵$R$的前两列。为了简便，将内参矩阵记为$A$</p><p>  $$<br>  s\left[\begin{aligned}&amp;u\&amp;v\&amp;1\end{aligned}\right]&#x3D;\left[\begin{aligned}&amp;\frac{f}{dX} &amp;-\frac{f\cot\theta}{dX}&amp; \ &amp;u_0\&amp;0 &amp;\frac{f}{dY\sin\theta}&amp; \ &amp;v_0\&amp;0 &amp;0&amp; \ &amp;1\end{aligned}\right]\left[\begin{aligned}&amp;R_1&amp; R_2&amp;&amp;T\end{aligned}\right]\left[\begin{aligned}&amp;U\&amp;V\&amp;1\end{aligned}\right]&#x3D;A[R_1\ \ R_2\ \ T]\left[\begin{aligned}&amp;U\&amp;V\&amp;1\end{aligned}\right]<br>  $$</p><p>  对于不同的图片，内参矩阵$A$为定值；对于同一张图片，内参矩阵$A$，外参矩阵$[R_1\ \ R_2\ \ T]$为定值；对于同一张图片上的单点，内参矩阵$A$，外参矩阵，尺度因子$s$为定值</p><p>  我们将$A[R_1\ \ R_2\ \ T]$记为矩阵$H$， $H$即为内参矩阵和外参矩阵的积，记矩阵$H$的三列为$[H_1,H_2,H_3]$，则有</p><p>  $$<br>  \left[\begin{aligned}&amp;u\&amp;v\&amp;1\end{aligned}\right]&#x3D;\frac{1}{s}H\left[\begin{aligned}&amp;U\&amp;V\&amp;1\end{aligned}\right]&#x3D;\frac{1}{s}\left[\begin{aligned}&amp;H_{11} &amp;H_{12}&amp; &amp;H_{13}\&amp;H_{21} &amp;H_{22}&amp; &amp;H_{23}\&amp;H_{31} &amp;H_{32}&amp; &amp;H_{33}\end{aligned}\right]\left[\begin{aligned}&amp;U\&amp;V\&amp;1\end{aligned}\right]<br>  $$</p><p>  利用上式，消去尺度因子$s$，可得</p><p>  $$<br>  \begin{aligned}u&amp;&#x3D;\frac{H_{11}U+H_{12}V+H_{13}}{H_{31}U+H_{32}V+H_{33}}\v&amp;&#x3D;\frac{H_{21}U+H_{22}V+H_{23}}{H_{31}U+H_{32}V+H_{33}}\end{aligned}<br>  $$</p><p>  此时，尺度因子$s$已经被消去，因此上式对于同一张图片上所有的角点均成立。$(u,v)$是像素坐标系下的标定板角点的坐标，$(U,V)$是世界坐标系下的标定板角点的坐标。通过图像识别算法，我们可以得到标定板角点的像素坐标$(u,v)$，又由于标定板的世界坐标系是人为定义好的，标定板上每一个格子的大小是已知的，我们可以计算得到世界坐标系下的$(U,V)$</p><p>  由这里的$H$是齐次矩阵，有8个独立未知元素。每一个标定板角点可以提供两个约束方程（$v,U,V$的对应关系、$u,U,V$的对应关系提供了两个约束方程），因此，当一张图片上的标定板角点数量等于4时，即可求得该图片对应的矩阵$H$。当一张图片上的标定板角点数量大于4时，利用最小二乘法回归最佳的矩阵$H$</p></li><li><p>求解内参矩阵</p><p>  我们已知了矩阵$H<br>  &#x3D;A[R_1\ \ R_2\ \ T]$，接下来需要求解相机的内参矩阵$A$。</p><p>  我们利用$R_1,R_2$作为旋转矩阵$R$的两列，存在单位正交的关系，即：</p><p>  $$<br>  R_1^TR_2&#x3D;0\R_1^TR_1&#x3D;R_2^TR_2&#x3D;I<br>  $$</p><p>  则由$H$和$R_1,R_2$的关系，可知</p><p>  $$<br>  R_1&#x3D;A^{-1}H_1\R_2&#x3D;A^{-1}H_2<br>  $$</p><p>  带入得到</p><p>  $$<br>  H_1^TA^{-T}A^{-1}H_2&#x3D;0\H_1^TA^{-T}A^{-1}H_1&#x3D;H_2^TA^{-T}A^{-1}H_2&#x3D;I<br>  $$</p><p>  另外，我们发现，上述两个约束方程中均存在矩阵$A^{-T}A^{-1}$。因此，我们记$A^{-T}A^{-1}&#x3D;B$，则$B$为对称阵。我们试图先求解出矩阵 $B$，通过矩阵 $B$再求解相机的内参矩阵 $A$。同时，为了简便，我们记相机内参矩阵$A$为</p><p>  $$<br>  A&#x3D;\left[\begin{aligned}&amp;\frac{f}{dX} &amp;-\frac{f\cot\theta}{dX}&amp; \ &amp;u_0&amp; &amp;0\&amp;0 &amp;\frac{f}{dY\sin\theta}&amp; \ &amp;v_0&amp; &amp;0\&amp;0 &amp;0&amp; \ &amp;1&amp; &amp;0\end{aligned}\right]&#x3D;\left[\begin{aligned}&amp;\alpha &amp;\gamma&amp; &amp;u_0\&amp;0 &amp;\beta&amp; &amp;v_0\&amp;0 &amp;0&amp; &amp;1\end{aligned}\right]<br>  $$</p><p>  则有</p><p>  $$<br>  A^{-1}&#x3D;\left[\begin{aligned}&amp;\frac{1}{\alpha} &amp;-\frac{\gamma}{\alpha\beta}&amp; &amp;\frac{\gamma v_0-\beta u_0}{\alpha\beta}\&amp;0 &amp;\frac{1}{\beta}&amp; &amp;-\frac{v_0}{\beta}\&amp;0 &amp;0&amp; &amp;1\end{aligned}\right]<br>  $$</p><p>  而$B$为</p><p>  $$<br>  B&#x3D;A^{-T}A^{-1}&#x3D;\left[\begin{aligned}&amp;B_{11} &amp;B_{12}&amp; &amp;B_{13}\&amp;B_{21} &amp;B_{22}&amp; &amp;B_{23}\&amp;B_{31} &amp;B_{32}&amp; &amp;B_{33}\end{aligned}\right]<br>  $$</p><p>  通过$R_1,R_2$正交的关系，需要计算$H_i^TBH_j$来得到最终的矩阵$B$，令</p><p>  $$<br>  \begin{aligned}v_{ij}&amp;&#x3D;[H_{1i}H_{1j}\ \ H_{1i}H_{2j}+H_{2i}H_{1j}\ \ H_{1i}H_{3j}+H_{3i}H_{1j}\ \ H_{2i}H_{3j}+H_{3i}H_{2j}\ \ H_{3i}H_{3j}]^T\ b&amp;&#x3D;[B_{11}\ \ B_{12}\ \ B_{22}\ \ B_{13}\ \ B_{23}\ \ B_{33}]^T\end{aligned}<br>  $$</p><p>  有$H_i^TBH_j&#x3D;v_{ij}b$，$R_1,R_2$正交方程简化为</p><p>  $$<br>  v_{12}^Tb&#x3D;0\v_{11}^Tb&#x3D;v_{22}^Tb&#x3D;I<br>  $$</p><p>  即</p><p>  $$<br>  \left[\begin{aligned}v_{12}^T&amp;\v_{11}^T-&amp;v_{22}^T\end{aligned}\right]b&#x3D;0<br>  $$</p><p>  由于矩阵$H$已知，矩阵$v&#x3D;\left[\begin{aligned}v_{12}^T&amp;\v_{11}^T-&amp;v_{22}^T\end{aligned}\right]$$\left[\begin{aligned}v_{12}^T&amp;\v_{11}^T-&amp;v_{22}^T\end{aligned}\right]$又全部由矩阵$H$的元素构成，因此矩阵$v$已知。</p><p>  此时，我们只要求解出向量$b$，即可得到矩阵$B$。每张标定板图片可以提供一个$vb&#x3D;0$的约束关系，该约束关系含有两个约束方程。但是，向量$b$有6个未知元素。因此，单张图片提供的两个约束方程是不足以解出来向量$b$。因此，我们只要取3张标定板照片，得到3个$vb&#x3D;0$的约束关系，即6个方程，即可求解向量$b$。当标定板图片的个数大于3时（事实上一般需要15到20张标定板图片），可采用最小二乘拟合最佳的向量$b$，并得到矩阵$B$。</p><p>  <img src="/./2022/06/24/%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A/latex.png" alt="function"></p><p>  根据矩阵$B$的元素和相机内参$\alpha,\beta,\gamma,u_0,v_0$的对应关系，可得到</p><p>  $$<br>  v_0&#x3D;\frac{B_{12}B_{13}-B_{11}B_{23}}{B_{11}B_{22}-B_{12}^2}\ \alpha&#x3D;\sqrt{\frac{1}{B_{11}}}\ \beta&#x3D;\sqrt{\frac{B_{11}}{B_{11}B_{22}-B_{12}^2}}\ \gamma&#x3D;-B_{12}\alpha^2\beta \ u_0&#x3D;\frac{\gamma v_0}{\beta}-B_{13}\alpha^2<br>  $$</p><p>  即可求得内参矩阵$A&#x3D;\left[\begin{aligned}&amp;\frac{1}{dX} &amp;-\frac{f\cot\theta}{dX}&amp; \ &amp;u_0&amp; &amp;0\&amp;0 &amp;\frac{1}{dY\sin\theta}&amp; \ &amp;v_0&amp; &amp;0\&amp;0 &amp;0&amp; \ &amp;1&amp; &amp;0\end{aligned}\right]&#x3D;\left[\begin{aligned}&amp;\alpha &amp;\gamma&amp; &amp;u_0\&amp;0 &amp;\beta&amp; &amp;v_0\&amp;0 &amp;0&amp; &amp;1\end{aligned}\right]$</p></li><li><p>求解外参矩阵</p><p>  对于同一个相机，相机的内参矩阵取决于相机的内部参数，无论标定板和相机的位置关系是怎么样的，相机的内参矩阵不变。这也正是在求解内参矩阵中，我们可以利用不同的图片（标定板和相机位置关系不同）获取的矩阵$H$，共同求解相机内参矩阵$A$的原因。</p><p>  但是，外参矩阵反映的是标定板和相机的位置关系。对于不同的图片，标定板和相机的位置关系已经改变，此时每一张图片对应的外参矩阵都是不同的。</p><p>  在关系$H<br>  &#x3D;A[R_1\ \ R_2\ \ T]$中，我们已经求解得到了矩阵$H$（对于同一张图片相同，对于不同的图片不同）、矩阵$A$（对于不同的图片都相同）。通过公式$[R_1\ \ R_2\ \ T]&#x3D;A^{-1}H$，即可求得每一张图片对应的外参矩阵$[R_1\ \ R_2\ \ T]$。</p><p>  注意，这里值得指出，完整的外参矩阵为$\left[\begin{aligned}&amp;R &amp;T\&amp;0 &amp;1\end{aligned}\right]$。但是，由于张正友标定板将世界坐标系的原点选取在棋盘格上，则棋盘格上任一点的物理坐标$W&#x3D;0$，将旋转矩阵的$R$的第三列$R_3$消掉，因此，$R_3$在坐标转化中并没有作用。但是$R_3$要使得$R$满足旋转矩阵的性质，即列与列之间单位正交，因此可以通过向量$R_1,R_2$的叉乘，即$R_3&#x3D;R_1\times R_2$计算得到。</p><p>  此时，相机的内参矩阵和外参矩阵均已得到。</p><p>  <strong>注：以上推导都是假设不存在畸变参数的情况下成立的</strong>。但是事实上，相机是存在畸变参数的，因此，张正友标定法还需要通过L-M算法对于参数进行迭代优化。</p></li></ul><h2 id="标定步骤"><a href="#标定步骤" class="headerlink" title="标定步骤"></a>标定步骤</h2><ol><li>准备一个张正友标定法的棋盘格，棋盘格大小已知，用相机对其进行不同角度的拍摄，得到一组图像</li><li>对图像中的特征点如标定板角点进行检测，得到标定板角点的像素坐标值，根据已知的棋盘格大小和世界坐标系原点，计算得到标定板角点的物理坐标值</li><li>求解内参矩阵与外参矩阵。根据物理坐标值和像素坐标值的关系，求出$H$矩阵，进而构造$v$矩阵，求解$B$矩阵，利用$B$矩阵求解相机内参矩阵$A$，最后求解每张图片对应的相机外参矩阵$\left[\begin{aligned}&amp;R&amp; t\&amp;0&amp;1\end{aligned}\right]$</li><li>求解畸变参数。利用$\hat{u},u,\hat{v},v$构造$D$矩阵，计算径向畸变参数</li><li>利用L-M（Levenberg-Marquardt）算法对上述参数进行优化</li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://blog.csdn.net/lql0716/article/details/71973318?locationNum=8&fps=1"></a></p><p><a href="https://blog.csdn.net/a083614/article/details/78579163">最详细、最完整的相机标定讲解_a083614的专栏-CSDN博客_相机标定</a></p><p><a href="https://zhuanlan.zhihu.com/p/30813733">相机标定究竟在标定什么？</a></p><p><a href="https://zhuanlan.zhihu.com/p/94244568">相机标定之张正友标定法数学原理详解（含python源码）</a></p>]]></content>
    
    
    <categories>
      
      <category>3D Basic</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Camera</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>start</title>
    <link href="/2022/06/24/start/"/>
    <url>/2022/06/24/start/</url>
    
    <content type="html"><![CDATA[<h1 id="关于此份pages"><a href="#关于此份pages" class="headerlink" title="关于此份pages"></a>关于此份pages</h1><br /><p>此Github Pages为本人记录3D学习以及各种杂项的bak。</p>]]></content>
    
    
    <categories>
      
      <category>misc</category>
      
    </categories>
    
    
    <tags>
      
      <tag>misc</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
