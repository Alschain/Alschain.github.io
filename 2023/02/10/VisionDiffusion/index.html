

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/avatar.jpg">
  <link rel="icon" href="/img/avatar.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Alschain">
  <meta name="keywords" content="">
  
    <meta name="description" content="Questions What is Diffusion Model? How diffusion models work? What can diffusion models do?  近期diffusion model在很多地方都被应用，如Tsinghua - Liu Yebin团队CVPR paper DiffuStereo，在此对扩散模型做一个简单的阅读，提出三个问题看在文档结尾能否给自已一">
<meta property="og:type" content="article">
<meta property="og:title" content="Diffusion Models in Vision">
<meta property="og:url" content="https://alschain.com/2023/02/10/VisionDiffusion/index.html">
<meta property="og:site_name" content="Alschain">
<meta property="og:description" content="Questions What is Diffusion Model? How diffusion models work? What can diffusion models do?  近期diffusion model在很多地方都被应用，如Tsinghua - Liu Yebin团队CVPR paper DiffuStereo，在此对扩散模型做一个简单的阅读，提出三个问题看在文档结尾能否给自已一">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://alschain.com/2023/02/10/VisionDiffusion/ddpm.png">
<meta property="og:image" content="https://alschain.com/2023/02/10/VisionDiffusion/Reparameterization.png">
<meta property="og:image" content="https://alschain.com/2023/02/10/VisionDiffusion/training.png">
<meta property="og:image" content="https://alschain.com/2023/02/10/VisionDiffusion/SR3.png">
<meta property="og:image" content="https://alschain.com/2023/02/10/VisionDiffusion/DSR.png">
<meta property="og:image" content="https://alschain.com/2023/02/10/VisionDiffusion/scores.png">
<meta property="og:image" content="https://alschain.com/2023/02/10/VisionDiffusion/sampling.png">
<meta property="og:image" content="https://alschain.com/2023/02/10/VisionDiffusion/DiffuStereo.png">
<meta property="article:published_time" content="2023-02-10T14:29:08.000Z">
<meta property="article:modified_time" content="2023-02-10T15:07:35.202Z">
<meta property="article:author" content="Alschain">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="diffusion">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://alschain.com/2023/02/10/VisionDiffusion/ddpm.png">
  
  
  
  <title>Diffusion Models in Vision - Alschain</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"alschain.com","root":"/","version":"1.9.2","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 5.4.2"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Alschain</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg_720.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Diffusion Models in Vision"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-02-10 22:29" pubdate>
          2023年2月10日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          7.3k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          61 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Diffusion Models in Vision</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h1><ol>
<li>What is Diffusion Model?</li>
<li>How diffusion models work?</li>
<li>What can diffusion models do?</li>
</ol>
<p>近期diffusion model在很多地方都被应用，如Tsinghua - Liu Yebin团队CVPR paper DiffuStereo，在此对扩散模型做一个简单的阅读，提出三个问题看在文档结尾能否给自已一些答案</p>
<h1 id="Surveys-amp-Blogs"><a href="#Surveys-amp-Blogs" class="headerlink" title="Surveys &amp; Blogs"></a>Surveys &amp; Blogs</h1><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2209.04747.pdf">Diffusion Models in Vision: A Survey</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2208.11970.pdf">Understanding Diffusion Models: A Unified Perspective</a>, by Calvin Luo, Google Brain</li>
<li><a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models? - Lil’s Blog</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/525106459">由浅入深了解Diffusion Model - ewrfcas’s Zhihu Article</a></li>
</ul>
<h1 id="DDPM"><a href="#DDPM" class="headerlink" title="DDPM"></a>DDPM</h1><p>| Denoising Diffusion Probabilistic Models</p>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>如 [1] [2] 等文章中阐述，diffusion model实际是一种生成模型。Model主要包含两个part，添加T轮的噪声部分 (Forward Pass) 以及对噪声图进行denoise的部分 (Reverse Pass)。</p>
<p><img src="/./2023/02/10/VisionDiffusion/ddpm.png" srcset="/img/loading.gif" lazyload alt="DDPM"></p>
<h2 id="Forward-Pass-Diffusion-Pass"><a href="#Forward-Pass-Diffusion-Pass" class="headerlink" title="Forward Pass (Diffusion Pass)"></a>Forward Pass (Diffusion Pass)</h2><p>Forward Pass即对图片$x_0 \sim q(x)$逐次添加噪声的过程，并最终呈现为完全的噪声分布。在添加噪声的过程中，每一个时刻$t$只与先前时刻$t-1$以及对应的Gaussian噪声有关。其中，$x_1, x_2, … x_t$均为与原图像同维度的latent code。即有</p>
<p>$$<br>q(x_t|x_{t-1}):&#x3D;\mathcal{N}(x_t;\sqrt{1-\beta_t}x_{t-1}, \beta_tI)<br>$$</p>
<p>其中$\beta \in (0,1)$为扩散率，实际中随着$t$的增大而增大。<br>整个过程可以被看成不断添加噪声的Markov过程，对于任意时刻$t$则有</p>
<p>$$<br>q(x_{1:t}|x_0):&#x3D;\Pi_{i&#x3D;1}^tq(x_i|x_{i-1})<br>$$</p>
<p>另外在[1]中也证明了，对于Gaussian分布，对于任意时刻$t$，若$\alpha_i:&#x3D;1-\beta_i, \bar{\alpha}_i:&#x3D; \Pi _{s&#x3D;1}^i\alpha_s$有</p>
<p>$$<br>q(x_t|x_0)&#x3D;\mathcal{N}(x_t;\sqrt{\bar{\alpha}_t}x_0,(1-\bar{\alpha}_t)I)<br>$$</p>
<p>这可以让我们通过$x_0,\beta$来快速获取时间$t$下的$x_t$，实际上即有</p>
<p>$$<br>x_t &#x3D; \sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon<br>$$</p>
<h2 id="Reverse-Pass"><a href="#Reverse-Pass" class="headerlink" title="Reverse Pass"></a>Reverse Pass</h2><p>与Forward Pass相反，在Reverse Pass中我们希望找出一个分布$p_\theta(x_{0:t})$，也由一个高斯核转移的Markov链来定义：</p>
<p>$$<br>p_\theta(x_{t-1}|x_t):&#x3D;\mathcal{N}(x_{t-1}; \mu_\theta(x_t,t), \sum_\theta(x_t,t))<br>$$</p>
<p>其中$\theta$为模型参数。对于任意时刻$t$则有</p>
<p>$$<br>p_\theta(x_{0:t}):&#x3D;p(x_t)\Pi_{i&#x3D;1}^tp_\theta(x_{t-1}|x_t)<br>$$</p>
<p>虽然无法得到分布$q(x_{t-1}|x_t)$，但在我们知道$x_0$的情况下，可以通过贝叶斯公式得到</p>
<p>$$<br>\begin{aligned}<br>q(x_{t-1}|x_t, x_0)&amp;&#x3D;q(x_t|x_{t-1},x_0)\frac{q(x_{t-1}|x_0)}{q(x_t|x_0)}\<br>&amp;&#x3D; … \<br>&amp;&#x3D;\mathcal{N}(x_{t-1};\tilde{\mu}(x_t,x_0),\tilde{\beta}_tI)<br>\end{aligned}<br>$$</p>
<p>利用上述式子可以将Reverse Pass转变为更易求解的Forward Pass，而其中的$\tilde\mu$可以由预测的Gaussian分布和$x_t$得到，在DDPM中有$\sum_\theta(x_t,t)&#x3D;\tilde\beta_t$，即可以表征当前时刻的分布。对于网络则需要学习出时刻$t$下的高斯噪声分布。</p>
<h2 id="Reparameterization"><a href="#Reparameterization" class="headerlink" title="Reparameterization"></a>Reparameterization</h2><p>和VAE类似，我们希望整个过程是可导的来完成训练的过程，而从一个高斯分布中进行采样这个过程是不可导的，而在Diffusion Model中每一步都充满了高斯分布的影响。因此和VAE一样，在Diffusion Models中也采用了重参数的技巧来使得采样过程变得可导。</p>
<p>$$<br>z&#x3D;\mu_\theta+\sigma_\theta \cdot \epsilon, \epsilon \in \mathcal{N}(0,I)<br>$$</p>
<p>对于一个高斯分布$z$来说，他可以被描述成上式的形式，利用上式的写法，其仍然是满足以均值为$\mu_\theta$、方差为$\sigma_\theta^2$的高斯分布，而随机性被转移到了$\epsilon$上。</p>
<p><img src="/./2023/02/10/VisionDiffusion/Reparameterization.png" srcset="/img/loading.gif" lazyload alt="Reparameterization"></p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>与VAE类似，在[1]中推导出其训练损失即为组合的KL散度和熵，在[1][2]等中都有较为详细的推导。<br><img src="/./2023/02/10/VisionDiffusion/training.png" srcset="/img/loading.gif" lazyload alt="Training"></p>
<p>而$L_0$则认为是一个离散化的过程，构建一个离散化的分段积分累乘。即在Sampling过程中$t&#x3D;&#x3D;1$获取生成图片时不需要在这一步继续添加噪声求解。实际上，网络学习的就是每一个时刻和下一个时刻</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>本质上，DDPM就是通过网络来学习到时刻$t$的噪声，并利用推导的公式来进行$t-1$时刻的生成，并不断重复此过程来获取最终生成的图片。而训练的时候，输入为$x_0$和噪声$\epsilon$的线性组合即$x_t$和时刻$t$，并通过对应的Loss进行约束训练。</p>
<p>但图像的质量往往取决于采样频率$T$的多少，而频率高了之后带来的问题是采样过程、训练过程十分漫长。在DDIM[4]中对这个问题进行了研究，并对此进行了加速。[4]中对$q(x_{t-1}|x_t, x_0)$进行推导分析，实际可以利用起始点和终点直接算出$q(x_{x}|x_k, x_0)$，即可以不像DDPM一样逐次采样，从而减少采样次数。 </p>
<h1 id="SR3"><a href="#SR3" class="headerlink" title="SR3"></a>SR3</h1><p>| Image Super-Resolution via Iterative Refinement</p>
<p>文章[5]则是利用了Diffusion来做超分的任务。</p>
<h2 id="Conditional-Reverse-Pass"><a href="#Conditional-Reverse-Pass" class="headerlink" title="Conditional Reverse Pass"></a>Conditional Reverse Pass</h2><p>和DDPM、DDIM不同，超分是一项特定的任务，因此在reverse pass中需要加入条件约束。因此在SR3中提出，原先在reverse pass中只需要输入$(x_t, t)$，而在超分任务中需要将LR image插值到HR image大小后，concat作为网络的输入。</p>
<p><img src="/./2023/02/10/VisionDiffusion/SR3.png" srcset="/img/loading.gif" lazyload alt="SR3"></p>
<p>除此之外，上图中的$p(\gamma)$实际上是噪声率。在DDPM中通过不断扩散的方式进行添加噪声，将时刻$t$加入到网络的输入中。而在SR3中则取消了时刻的输入，而是直接改为将当前的噪声率$\gamma$embedding。而这个$\gamma$在训练过程中实际上是对间隔$[\bar{\alpha}_{t-1}, \bar{\alpha}_t]$的一个均匀采样，与DDPM直接取$\bar{\alpha}_t$不同。</p>
<h2 id="Furthur-Thoughts"><a href="#Furthur-Thoughts" class="headerlink" title="Furthur Thoughts"></a>Furthur Thoughts</h2><p>利用SR3的思路，实际上对于Image2Image的任务，都可以通过类似这种形式简单粗暴地改写reverse pass。如对于deblur，也可以通过这种方法，将noise图片作为condition一起输入到网络中，作为reverse阶段的输入。</p>
<h1 id="DSR-Deblur"><a href="#DSR-Deblur" class="headerlink" title="DSR - Deblur"></a>DSR - Deblur</h1><p>| Deblurring via Stochastic Refinement</p>
<p><img src="/./2023/02/10/VisionDiffusion/DSR.png" srcset="/img/loading.gif" lazyload alt="DSR"></p>
<h2 id="Split-Branch"><a href="#Split-Branch" class="headerlink" title="Split Branch"></a>Split Branch</h2><p>如上图的overview，在这篇CVPR的deblur文章中，对于模糊图片的处理采用了两个模块：一个是initial预测deblurred的模块，另一个则是使用diffusion来预测残差的模块。最终将两个output结合生成deblurred image。</p>
<h2 id="Miscs"><a href="#Miscs" class="headerlink" title="Miscs"></a>Miscs</h2><p>另外与DDPM等不同的是，DSR仍然采用的和SISR相似的策略，在patch上训练，在image上inference.而由于每一次的采样都具有随机性，为了让结果更为稳定，文章中采用多次重建取均值的方式来提升最终的效果。</p>
<h1 id="Back-to-NCSN-Score-Based-Modeling"><a href="#Back-to-NCSN-Score-Based-Modeling" class="headerlink" title="Back to NCSN - Score-Based Modeling"></a>Back to NCSN - Score-Based Modeling</h1><p>| Generative Modeling by Estimating Gradients of the Data Distribution</p>
<p><img src="/./2023/02/10/VisionDiffusion/scores.png" srcset="/img/loading.gif" lazyload alt="Scores"></p>
<h2 id="Score-and-Generation"><a href="#Score-and-Generation" class="headerlink" title="Score and Generation"></a>Score and Generation</h2><p>实际上这一篇NCSN(Noise Coinditional Score Network, or Score-Based Modleing)[7]比DDPM更早将扩散模型作为生成模型来使用。在NCSN中，score实际上就是数据分布的梯度，利用这个score可以让我们从一个噪声基于Langevin Dynamics采样（实际上DDPM的采样也是类似这种形式），来生成最终的image。文章提出由于低密度区域的数据分布的score其实很难预测，这会导致很难或无法通过这些预测很差的score来生成最终的数据分布。因此NCSN提出，可以通过在数据中加入Gaussian噪声的方式，来扩大原先高密度的区域，这样能够使低密度区域相比与先前更容易预测score。由于添加了噪声会导致数据分布与原先不大一致，因此添加噪声的强弱等也属于一个tradeoff，需要让其不断增强对低密度区域的预测，但也要同时尽量保持希望的数据分布。</p>
<p>若$p_\theta(x)$为概率分布，实际上score即为$\nabla\log p_\theta(x)$，即概率的对数梯度。score的趋势即指向数据密度更高的方向，如上图所示。生成的过程即我们在空间中的点，每一个点通过Langevin Dynamics采样来不断逼近目标数据。即</p>
<p>$$<br>x_t\leftarrow x_{t-1}+\frac{\epsilon}{2}\nabla\log p_\theta(x_t)+\sqrt{\epsilon}z_t<br>$$</p>
<p><img src="/./2023/02/10/VisionDiffusion/sampling.png" srcset="/img/loading.gif" lazyload alt="Sampling"></p>
<p>实际上上述采样的过程即为分L步，期间每一步确定了noise level和步长，然后在这每一步中再进行T次的Langevin Dynamics采样。实际上在T&#x3D;1的时候，和DDPM中的采样方式是一致的。</p>
<h2 id="How-to-Get-Scores"><a href="#How-to-Get-Scores" class="headerlink" title="How to Get Scores"></a>How to Get Scores</h2><p>实际上估计score就是估计每一次添加的噪声（实际上score和噪声相比只差一个scale，基于添加的Gaussian噪声）。</p>
<p>在文章[8]（非常solid的一篇paper）中，将diffusion model与SDE的关系进行了更为详细的阐述，对连续形式的diffusion pass和reverse pass进行了构建说明，并将NCSN和DDPM进行了整合。实际上，NCSN要得到最终的一张纯噪声图，由于是在原图上直接进行的添加，因此需要非常大的噪声来掩盖原图的信息；而DDPM实际上是通过对原图进行比例控制来达到相同的目的。</p>
<h1 id="DiffuStereo"><a href="#DiffuStereo" class="headerlink" title="DiffuStereo"></a>DiffuStereo</h1><p>| DiffuStereo: High Quality Human Reconstruction via Diffusion-based Stereo Using Sparse Cameras</p>
<p><img src="/./2023/02/10/VisionDiffusion/DiffuStereo.png" srcset="/img/loading.gif" lazyload alt="DiffuStereo"></p>
<h2 id="Diffusion-in-DiffuStereo"><a href="#Diffusion-in-DiffuStereo" class="headerlink" title="Diffusion in DiffuStereo"></a>Diffusion in DiffuStereo</h2><p>在DiffuStereo一文中，作者利用diffusion model来对disparity进行refine。其中diffusion model不是直接将disparity map进行refine，而是将其与gt的残差作为model的输入。</p>
<p>实际上，DiffuStereo中的diffusion和SR3类似，也是一种conditional diffusion。DiffuStereo中利用了origin image、warped image、flow map（利用init flow和当前时刻的residual计算）和direction map一起作为输入来当作diffusion model的condition。和DDPM等不同的是，DiffuStereo的采样过程中扩散率是线性增加的。另外，对于initial的输入，DiffuStereo还做了一个global feature，将其应用于diffusion model的各个时刻。</p>
<p>$$<br>L_t&#x3D;\mathbb{E}_{y_0,\epsilon,t}[||f_\theta((1-\gamma_t)y_0+\gamma_t\epsilon,s,t)-y_0||^2]<br>$$</p>
<p>在Supp. Mat.中，diffusion model的训练损失是每一个时刻输出与$y_0$的$L_2$损失，见上式。和SR3类似，只是将模型直接预测噪声转成了预测残差值，即当前时刻的观测与噪声的线性组合。</p>
<p>Diffusion model在DiffuStereo中的作用就是改进flow map(disparity map)，它是一个独立的模块，理论上可以提供给所有的stereo来做refine。但原文并没有将这个module在别的stereo method进行实验，因此无法下结论是否这个模块在stereo method上都能够对结果有进一步提升，但是一个可以尝试的思路。</p>
<h1 id="End-of-this-Doc"><a href="#End-of-this-Doc" class="headerlink" title="End of this Doc"></a>End of this Doc</h1><ol>
<li>What is Diffusion Model?</li>
</ol>
<p>扩散模型实际上和GAN类似是一种生成模型，输入噪声图经过model得出需要的结果。</p>
<ol start="2">
<li>How diffusion models work?</li>
</ol>
<p>扩散模型通过定义添加噪声的forward pass&#x2F;diffusion pass和denoise的reverse pass以及相关的loss和数学推导来完成模型的训练，训练的model实际上是用于预测当前时刻&#x2F;噪声率下的噪声，并以此不断迭代最终生成需要的image的过程。<br>和GAN相比，GAN需要训练生成器和判别器两个模型，整个过程相对于diffusion model只需要训练一个过程而言更复杂且不稳定。而在生成任务中diffusion model能够达到很好的效果。</p>
<ol start="3">
<li>What can diffusion models do?</li>
</ol>
<p>In vision，扩散模型已经有研究人员们用于做图片生成、超分、去噪、甚至于检测(DiffusionDet)等一系列任务中。总体而言本质上需要我们定义好reverse的过程以及对应的condition，可以在对应的任务进行尝试。在多模态领域，diffusion model也有非常出色的表现。</p>
<ol start="4">
<li>Pros &amp; Cons</li>
</ol>
<p>如上述问题中所回答的，扩散模型可以高效的解决GAN等生成模型训练不稳定的问题，并且结构易于设计和实现。但同时，其依赖于高斯噪声假设来进行数学推理，可能会导致与真实场景的噪声不匹配。另外在inference过程中，由于需要经过多个采样步骤，和GAN相比其推理时间会更长。</p>
<ol start="5">
<li>What are the differences between diffusion models and recurrent U-Net?</li>
</ol>
<p>从inference pass而言，diffusion model输入$(x_t,t[\gamma, or else, …])$，然后将输出不断的当成输入继续送入网络。而对于recurrent U-Net来说，也是将得到的结果不断在U-Net中循环计算。但不同的是，diffusion model还需要知道当前的时刻$t$或是噪声率$\gamma$等相关的参数，然后在当前步骤中进行下一个时刻的求解；而不断的重复U-Net仅仅是对输入时刻的数据进行相同的求解。正如DDIM中阐述的，diffusion model的本质可以看成是一个SDE的求解的离散化过程，而在单一的不断重复的U-Net中实际上并不能改变其求解方向。</p>
<ol start="6">
<li>Can diffusion models be used in denoising?</li>
</ol>
<p>如DSR等工作已经将diffusion model用于denoise，但要想生成高质量的图片，需要经过更多步骤的采样，而对于我们实时性的要求在目前而言显然还是不能够满足。</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] <a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v37/sohl-dickstein15.pdf">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</a>, ICLR 2015</p>
<p>[2] <a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf">Denoising Diffusion Probabilistic Models</a>, NeurIPS 2020</p>
<p>[3] <a target="_blank" rel="noopener" href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73">Understanding Variational Autoencoders (VAEs)</a></p>
<p>[4] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.02502.pdf">Denoising Diffusion Implicit Models</a>, ICLR 2021</p>
<p>[5] <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9887996">Image Super-Resolution via Iterative Refinement</a>, T-PAMI 2022</p>
<p>[6] <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Whang_Deblurring_via_Stochastic_Refinement_CVPR_2022_paper.pdf">Deblurring via Stochastic Refinement</a>, CVPR 2022</p>
<p>[7] <a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf">Generative Modeling by Estimating Gradients of the Data Distribution</a>, NeurIPS 2019</p>
<p>[8] <a target="_blank" rel="noopener" href="https://openreview.net/pdf/ef0eadbe07115b0853e964f17aa09d811cd490f1.pdf">Score-Based Generative Modeling through Stochastic Differential Equations</a>, ICLR 2021</p>
<p>[9] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.08000.pdf">DiffuStereo: High Quality Human Reconstruction via Diffusion-based Stereo Using Sparse Cameras</a>, ECCV 2022</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/diffusion/" class="category-chain-item">diffusion</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/deep-learning/">#deep learning</a>
      
        <a href="/tags/diffusion/">#diffusion</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Diffusion Models in Vision</div>
      <div>https://alschain.com/2023/02/10/VisionDiffusion/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Alschain</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年2月10日</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/07/06/stereovision/" title="Stereo Vision">
                        <span class="hidden-mobile">Stereo Vision</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.1/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
